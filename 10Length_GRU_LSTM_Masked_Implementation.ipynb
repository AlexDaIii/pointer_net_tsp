{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10Length_GRU_LSTM_Masked_Implementation.ipynb","version":"0.3.2","provenance":[{"file_id":"1kAfQvu-B_aUHdUMF9AOqL-TiHcIZISoo","timestamp":1555859258409},{"file_id":"1O1ZkUb5S_IzHePHQEUuTa0qkjFmTrZtI","timestamp":1555449701261},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/seq2seq_translation_tutorial.ipynb","timestamp":1555257061838}],"collapsed_sections":["bTfLcxSCc2Jr"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Hiwi182oc9qS","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"IG21wvnzknsT","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","from os.path import join\n","\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NfCHYJNcgLw","colab_type":"text"},"source":["# Data processing"]},{"cell_type":"code","metadata":{"id":"AE7XxbKoYCKc","colab_type":"code","outputId":"33390a87-3426-4620-a315-86d2ee4813ae","executionInfo":{"status":"ok","timestamp":1556852378861,"user_tz":240,"elapsed":28758,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","path = \"/content/gdrive/My Drive/Colab Notebooks/\"\n","drive.mount('/content/gdrive')\n","!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/tsp_20_test.txt ."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yTbgo9nSCOUo","colab_type":"code","outputId":"7d87c18b-eafa-46dc-f48b-9355ff9299af","executionInfo":{"status":"ok","timestamp":1556852022346,"user_tz":240,"elapsed":17751,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# copied from https://github.com/keon/pointer-networks/blob/master/tsp_data.py\n","\n","import math\n","import numpy as np\n","import random\n","import itertools\n","from sklearn.model_selection import train_test_split\n","\n","class Tsp:\n","    def next_batch(self, batch_size=1):\n","        X, Y = [], []\n","        for b in range(batch_size):\n","#             print(\"preparing dataset... %s/%s\" % (b, batch_size))\n","            points = self.generate_data()\n","            solved = self.solve_tsp_dynamic(points)\n","            X.append(points), Y.append(solved)\n","        return np.asarray(X), np.asarray(Y)\n","\n","    def length(self, x, y):\n","        return (math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2))\n","\n","    def solve_tsp_dynamic(self, points):\n","        # calc all lengths\n","        all_distances = [[self.length(x, y) for y in points] for x in points]\n","        # initial value - just distance from 0 to\n","        # every other point + keep the track of edges\n","        A = {(frozenset([0, idx+1]), idx+1): (dist, [0, idx+1])\n","             for idx, dist in enumerate(all_distances[0][1:])}\n","        cnt = len(points)\n","        for m in range(2, cnt):\n","            B = {}\n","            for S in [frozenset(C) | {0}\n","                      for C in itertools.combinations(range(1, cnt), m)]:\n","                for j in S - {0}:\n","                    B[(S, j)] = min([(A[(S-{j}, k)][0] + all_distances[k][j],\n","                                      A[(S-{j}, k)][1] + [j])\n","                                     for k in S if k != 0 and k != j])\n","            A = B\n","        res = min([(A[d][0] + all_distances[0][d[1]], A[d][1])\n","                   for d in iter(A)])\n","        return res[1]\n","\n","    def generate_data(self, N=10):\n","        radius = 1\n","        rangeX = (0, 10)\n","        rangeY = (0, 10)\n","        qty = N\n","\n","        deltas = set()\n","        for x in range(-radius, radius+1):\n","            for y in range(-radius, radius+1):\n","                if x*x + y*y <= radius*radius:\n","                    deltas.add((x, y))\n","\n","        randPoints = []\n","        excluded = set()\n","        i = 0\n","        while i < qty:\n","            x = random.randrange(*rangeX)\n","            y = random.randrange(*rangeY)\n","            if (x, y) in excluded:\n","                continue\n","            randPoints.append((x, y))\n","            i += 1\n","            excluded.update((x+dx, y+dy) for (dx, dy) in deltas)\n","        return randPoints\n","    \n","def read_from_file():\n","    with open(\"tsp_20_test.txt\") as file:\n","        X = []\n","        Y = []\n","\n","        for line in file:\n","            x, y = line.strip().split(\" output \")\n","            x = x.strip().split()\n","            x = [float(i) for i in x]\n","            x = [[x[i], x[i+1]] for i in range(0,len(x)-1,2)]\n","            y = y.strip().split()\n","            y = [int(i)-1 for i in y[:-1]]\n","            X.append(x)\n","            Y.append(y)\n","        X = np.array(X)\n","        Y = np.array(Y)\n","        return X, Y\n","    \n"," \n","    \n","p = Tsp()\n","X, Y = p.next_batch(1000)\n","# X, Y = read_from_file()\n","# X, Y = X[:100], Y[:100]\n","print(X.shape, Y.shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(1000, 10, 2) (1000, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D265jt86rMYw","colab_type":"code","colab":{}},"source":["class TSPDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.Y = Y\n","        self.data_size = X.shape[0]\n","        self.label_size = Y.shape[1]\n","        \n","    def __len__(self):\n","        return self.data_size\n","    def __getitem__(self, idx):\n","        input_tensor = torch.from_numpy(self.X[idx]).float()   \n","        target_tensor = torch.from_numpy(self.Y[idx]).long()\n","\n","        return input_tensor, target_tensor\n","\n","tspdataset = TSPDataset(X, Y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyTajeq4cyuc","colab_type":"text"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"9R5smuAEkEeC","colab_type":"code","colab":{}},"source":["class Encoder_GRU(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Encoder_GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.batch_size = batch_size\n","        self.relu = nn.ReLU()\n","        self.n_layers = 1\n","\n","        \n","        self.embedding = nn.Linear(2, self.hidden_size)\n","        self.rnn = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.h_x = nn.Parameter(torch.zeros(1), requires_grad=False)\n","        \n","    def forward(self, input, hidden):\n","        embedded = self.relu(self.embedding(input)).permute(1, 0, 2)\n","        output, hidden = self.rnn(embedded, hidden)\n","        return output.permute(1,0,2) , hidden, embedded.permute(1,0,2)\n","    \n","    def init_hidden(self):\n","        h_x = self.h_x.unsqueeze(0).unsqueeze(0).repeat(self.n_layers, self.batch_size, self.hidden_size)\n","\n","        return h_x\n","\n","class Encoder_LSTM(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Encoder_LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.batch_size = batch_size\n","        self.relu = nn.ReLU()\n","        self.n_layers = 1\n","        \n","        self.embedding = nn.Linear(2, self.hidden_size)\n","        self.rnn = nn.LSTM(self.hidden_size, self.hidden_size, self.n_layers)\n","        \n","        self.c_x = nn.Parameter(torch.zeros(1), requires_grad=False)\n","        self.h_x = nn.Parameter(torch.zeros(1), requires_grad=False)\n","        \n","    def forward(self, input, hidden, context):\n","        embedded = self.relu(self.embedding(input)).permute(1, 0, 2)\n","        output, (hidden, context) = self.rnn(embedded, (hidden, context))\n","        return output.permute(1,0,2) , hidden, embedded.permute(1,0,2), context\n","    \n","    def init_hidden(self):\n","        c_x = self.c_x.unsqueeze(0).unsqueeze(0).repeat(self.n_layers, self.batch_size, self.hidden_size)\n","        h_x = self.h_x.unsqueeze(0).unsqueeze(0).repeat(self.n_layers, self.batch_size, self.hidden_size)\n","\n","        return h_x, c_x\n","\n","class Attention_GRU(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention_GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        self.W_hidden = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_encoded = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.V = nn.Parameter(torch.FloatTensor(self.hidden_size), requires_grad=True)\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        nn.init.uniform_(self.V, -1, 1)\n","    \n","    def forward(self, input, hidden):\n","        encoder_outputs = self.W_encoded(input).permute(1,0,2)\n","        hidden = self.W_hidden(hidden)\n","        summed = self.tanh(encoder_outputs + hidden).permute(0,2,1)\n","        V = self.V.unsqueeze(0).expand(summed.size(0), -1).unsqueeze(1)\n","        attn_weights = torch.bmm(V, summed).squeeze().permute(1,0)\n","        alpha = self.softmax(attn_weights).unsqueeze(1)\n","        ct = torch.bmm(alpha, input).squeeze()\n","        return alpha.squeeze(), ct, attn_weights\n","\n","class Attention_LSTM(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention_LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.n_layers = 1\n","        \n","        self.W_encoded = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_hidden_context = nn.Linear(self.hidden_size * 1, self.hidden_size)\n","        self.W_average = nn.Linear(self.n_layers, 1)\n","        self.V = nn.Parameter(torch.FloatTensor(self.hidden_size), requires_grad=True)\n","        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        nn.init.uniform_(self.V, -1, 1)\n","    \n","    def forward(self, input, hidden, context, mask):\n","        encoder_outputs = self.W_encoded(input).permute(1,0,2)\n","#         hidden_context = torch.cat((hidden, context), dim=2)\n","        hidden_context = hidden\n","        hidden_context = self.W_hidden_context(hidden_context)\n","#         hidden_context = torch.mean(hidden_context, dim=0)\n","        hidden_context = hidden_context.permute(2,1,0)\n","        hidden_context = self.W_average(hidden_context).permute(2,1,0)\n","        summed = self.tanh(encoder_outputs + hidden_context).permute(0,2,1)\n","        V = self.V.unsqueeze(0).expand(summed.size(0), -1).unsqueeze(1)\n","        attn_weights = torch.bmm(V, summed).squeeze().permute(1,0)\n","        if (len(attn_weights[mask]) > 0):\n","            attn_weights[mask] = self.inf[mask]\n","        alpha = self.softmax(attn_weights).unsqueeze(1)\n","        ct = torch.bmm(alpha, input).squeeze()\n","        return alpha.squeeze(), ct, attn_weights\n","    \n","    def init_inf(self, size):\n","        self.inf = self._inf.unsqueeze(1).expand(*size)\n","        \n","    \n","class Decoder_GRU(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Decoder_GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        self.rnn = nn.GRU(self.hidden_size*2, self.hidden_size)\n","        self.attention = Attention_GRU(self.hidden_size)\n","        \n","    def forward(self, input, hidden, encoder_outputs, embedded):\n","        outputs = []\n","        pointers = []\n","                \n","        for _ in range(encoder_outputs.size(1)):\n","            alpha, ct, attn_weights = self.attention(encoder_outputs, hidden)\n","            w = torch.cat((input, ct),1).unsqueeze(0)\n","            _, hidden = self.rnn(w, hidden)\n","            _, topi = alpha.topk(1)\n","            topi = topi.squeeze().detach()\n","            input_ = []\n","            for j, i in enumerate(topi):\n","                input_.append(embedded[j,i,:].unsqueeze(0))\n","            input = torch.cat(input_)\n","            outputs.append(attn_weights)\n","            pointers.append(topi)\n","        \n","        return torch.stack(outputs,1), torch.stack(pointers,1)\n","        \n","class Decoder_LSTM(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Decoder_LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.n_layers = 1\n","        \n","        self.rnn = nn.LSTM(self.hidden_size*2, self.hidden_size, self.n_layers)\n","        self.attention = Attention_LSTM(self.hidden_size)\n","        \n","        self.mask = nn.Parameter(torch.ones(1), requires_grad=False)\n","        \n","    def forward(self, input, hidden, encoder_outputs, embedded, context):\n","        outputs = []\n","        pointers = []\n","        \n","        mask = self.mask.repeat(encoder_outputs.size(1)).unsqueeze(0).repeat(encoder_outputs.size(0), 1)\n","        self.attention.init_inf(mask.size())\n","                \n","        for _ in range(encoder_outputs.size(1)):\n","            alpha, ct, attn_weights = self.attention(encoder_outputs, hidden, context, torch.eq(mask, 0))\n","            w = torch.cat((input, ct),1).unsqueeze(0)\n","            _, (hidden, context) = self.rnn(w, (hidden, context))\n","            \n","            masked_alpha = alpha * mask\n","            \n","            _, topi = masked_alpha.max(1)\n","            input_ = []\n","            for j, i in enumerate(topi):\n","                input_.append(embedded[j,i,:].unsqueeze(0))\n","                mask[j,i] = 0.\n","            input = torch.cat(input_)\n","            outputs.append(alpha)\n","            pointers.append(topi)\n","        \n","        return torch.stack(outputs,1), torch.stack(pointers,1)\n","\n","class Pointer_GRU(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Pointer_GRU, self).__init__()\n","        self.encoder = Encoder_GRU(hidden_size, batch_size)\n","        self.decoder = Decoder_GRU(hidden_size)  \n","        self.decoder_input0 = nn.Parameter(torch.FloatTensor(batch_size, hidden_size), requires_grad=False)\n","        \n","        nn.init.uniform_(self.decoder_input0, -1, 1)\n","        \n","    def forward(self, input):\n","        eh0 = self.encoder.init_hidden()\n","        output, hidden, embedded = self.encoder(input, eh0)\n","        o, p = self.decoder(self.decoder_input0, hidden, output, embedded)\n","        return o, p\n","    \n","class Pointer_LSTM(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Pointer_LSTM, self).__init__()\n","        self.encoder = Encoder_LSTM(hidden_size, batch_size)\n","        self.decoder = Decoder_LSTM(hidden_size)  \n","        self.decoder_input0 = nn.Parameter(torch.FloatTensor(batch_size, hidden_size), requires_grad=False)\n","        \n","        nn.init.uniform_(self.decoder_input0, -1, 1)\n","        \n","    def forward(self, input):\n","        eh0, ec0 = self.encoder.init_hidden()\n","        output, hidden, embedded, context = self.encoder(input, eh0, ec0)\n","        o, p = self.decoder(self.decoder_input0, hidden, output, embedded, context)\n","        return o, p\n","        \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTfLcxSCc2Jr","colab_type":"text"},"source":["# GRU Training\n"]},{"cell_type":"code","metadata":{"id":"L7EhrRaS8CUi","colab_type":"code","outputId":"f05d39dd-0d64-4b8d-a1a1-2273dc33588a","executionInfo":{"status":"ok","timestamp":1556853581794,"user_tz":240,"elapsed":146480,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":884}},"source":["batch_size = 50\n","dataloader = DataLoader(tspdataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","hidden_size = 256\n","\n","pointer_net = Pointer_GRU(hidden_size, batch_size)\n","\n","if torch.cuda.is_available():\n","    USE_CUDA = True\n","    print('Using GPU, %i devices.' % torch.cuda.device_count())\n","    pointer_net.cuda()\n","    net = torch.nn.DataParallel(pointer_net, device_ids=range(torch.cuda.device_count()))\n","    cudnn.benchmark = True\n","else:\n","    print(\"Not using GPU\")\n","    USE_CUDA = False\n","    \n","lr = 1e-3\n","    \n","CCE = torch.nn.CrossEntropyLoss()\n","pointer_optim = optim.RMSprop(filter(lambda p: p.requires_grad,\n","                            pointer_net.parameters()),\n","                     lr=lr)\n","    \n","def train(epochs=10, step=1000, name=\"model\"):\n","    losses = []\n","    batch_accs = []\n","    eps = []\n","    Dist = []\n","    \n","    for epoch in range(epochs):\n","        for j, (input_tensor, target_tensor) in enumerate(dataloader):\n","           \n","            if USE_CUDA:\n","                input_tensor = input_tensor.cuda()\n","                target_tensor = target_tensor.cuda()\n","\n","            o, p = pointer_net(input_tensor)\n","            loss = CCE(o, target_tensor)\n","                        \n","            if ((len(dataloader))*(epoch) + (j+1)) % step == 0:\n","                \n","                compare = target_tensor == p\n","                acc_sum = compare.sum(dim=1)\n","                correct = 0.\n","                dist = compare[compare == 0].size()[0]\n","                for i in range(batch_size):\n","                    if acc_sum[i].item() == target_tensor.size(1):\n","                        correct += 1\n","                acc = correct / batch_size * 100\n","                batch_accs.append(acc)\n","                losses.append(loss.item())\n","                eps.append((epoch+1))\n","                Dist.append(dist)\n","                print(\"epoch {}: batch: {}/{} dist: {} loss: {}\".format(epoch+1,j+1,len(dataloader),dist,loss.item()), p[0], target_tensor[0])\n","                to_save = {\"pointer_net_state_dict\" : pointer_net.state_dict(), \"pointer_optim_state_dict\" : pointer_optim.state_dict(), \"epoch\" : epoch, \"j\" : j}\n","                torch.save(to_save, join(path, \"pointer_net_\"+name+\"_\"+str(epoch)+\".pt\"))\n","            \n","            pointer_optim.zero_grad()\n","            loss.backward()\n","#             torch.nn.utils.clip_grad_norm_(pointer_net.parameters(), 0.5)\n","            pointer_optim.step()\n","            \n","#     for l, a in zip(losses, batch_accs):\n","#         print(l,a)\n","    return losses, eps, Dist\n","\n","losses_gru, epochs_gru, dist_gru = train(100, step=20*2, name=\"gru\")\n","            "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using GPU, 1 devices.\n","epoch 2: batch: 20/20 dist: 398 loss: 1.9843437671661377 tensor([0, 1, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 1, 9, 4, 6, 3, 2, 8, 7, 5], device='cuda:0')\n","epoch 4: batch: 20/20 dist: 396 loss: 1.9737744331359863 tensor([0, 2, 4, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 1, 4, 3, 9, 8, 5, 7, 6, 2], device='cuda:0')\n","epoch 6: batch: 20/20 dist: 400 loss: 1.9741313457489014 tensor([0, 1, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 3, 4, 6, 1, 2, 9, 7, 8, 5], device='cuda:0')\n","epoch 8: batch: 20/20 dist: 402 loss: 1.9754364490509033 tensor([0, 2, 2, 2, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 6, 9, 1, 4, 5, 8, 3, 2, 7], device='cuda:0')\n","epoch 10: batch: 20/20 dist: 401 loss: 1.9734270572662354 tensor([0, 2, 3, 3, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 7, 9, 1, 6, 5, 3, 8, 2, 4], device='cuda:0')\n","epoch 12: batch: 20/20 dist: 405 loss: 1.9719921350479126 tensor([0, 1, 1, 1, 3, 3, 4, 4, 4, 4], device='cuda:0') tensor([0, 6, 4, 7, 3, 1, 9, 8, 2, 5], device='cuda:0')\n","epoch 14: batch: 20/20 dist: 394 loss: 1.9743889570236206 tensor([0, 2, 2, 3, 3, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 9, 4, 2, 3, 8, 7, 1, 6, 5], device='cuda:0')\n","epoch 16: batch: 20/20 dist: 400 loss: 1.9706950187683105 tensor([0, 3, 3, 3, 3, 3, 6, 6, 6, 6], device='cuda:0') tensor([0, 6, 1, 3, 4, 5, 9, 7, 2, 8], device='cuda:0')\n","epoch 18: batch: 20/20 dist: 399 loss: 1.9723334312438965 tensor([0, 3, 6, 6, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 7, 2, 1, 4, 5, 8, 6, 3, 9], device='cuda:0')\n","epoch 20: batch: 20/20 dist: 397 loss: 1.9749926328659058 tensor([0, 1, 1, 1, 1, 1, 1, 1, 3, 3], device='cuda:0') tensor([0, 5, 6, 3, 8, 2, 7, 4, 1, 9], device='cuda:0')\n","epoch 22: batch: 20/20 dist: 404 loss: 1.972906470298767 tensor([0, 5, 5, 5, 5, 5, 5, 7, 7, 7], device='cuda:0') tensor([0, 7, 5, 4, 2, 6, 3, 9, 1, 8], device='cuda:0')\n","epoch 24: batch: 20/20 dist: 398 loss: 1.965388536453247 tensor([0, 2, 3, 3, 3, 3, 6, 6, 6, 6], device='cuda:0') tensor([0, 9, 4, 5, 8, 7, 3, 6, 2, 1], device='cuda:0')\n","epoch 26: batch: 20/20 dist: 399 loss: 1.966539740562439 tensor([0, 5, 2, 2, 2, 2, 2, 2, 2, 5], device='cuda:0') tensor([0, 1, 8, 5, 2, 4, 9, 6, 7, 3], device='cuda:0')\n","epoch 28: batch: 20/20 dist: 404 loss: 1.9572588205337524 tensor([0, 2, 3, 7, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 1, 6, 2, 7, 4, 5, 3, 8, 9], device='cuda:0')\n","epoch 30: batch: 20/20 dist: 397 loss: 1.9685086011886597 tensor([0, 2, 2, 2, 2, 2, 4, 4, 4, 4], device='cuda:0') tensor([0, 3, 8, 1, 2, 9, 7, 4, 6, 5], device='cuda:0')\n","epoch 32: batch: 20/20 dist: 400 loss: 1.9750864505767822 tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 3, 2, 9, 1, 7, 5, 6, 4, 8], device='cuda:0')\n","epoch 34: batch: 20/20 dist: 400 loss: 1.9736770391464233 tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 2, 7, 4, 8, 9, 3, 1, 5, 6], device='cuda:0')\n","epoch 36: batch: 20/20 dist: 393 loss: 1.969808578491211 tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 6, 5, 3, 4, 1, 8, 2, 9, 7], device='cuda:0')\n","epoch 38: batch: 20/20 dist: 394 loss: 1.9670050144195557 tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 7, 4, 9, 8, 6, 3, 1, 5, 2], device='cuda:0')\n","epoch 40: batch: 20/20 dist: 401 loss: 1.9736582040786743 tensor([0, 1, 2, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 2, 1, 3, 6, 5, 8, 9, 7, 4], device='cuda:0')\n","epoch 42: batch: 20/20 dist: 399 loss: 1.9702767133712769 tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 8, 1, 7, 5, 2, 4, 3, 6, 9], device='cuda:0')\n","epoch 44: batch: 20/20 dist: 398 loss: 1.9680529832839966 tensor([0, 1, 1, 1, 1, 4, 4, 1, 1, 1], device='cuda:0') tensor([0, 3, 4, 6, 9, 2, 1, 8, 5, 7], device='cuda:0')\n","epoch 46: batch: 20/20 dist: 398 loss: 1.9684734344482422 tensor([0, 1, 1, 1, 1, 1, 5, 5, 5, 5], device='cuda:0') tensor([0, 4, 1, 3, 9, 2, 7, 6, 5, 8], device='cuda:0')\n","epoch 48: batch: 20/20 dist: 395 loss: 1.9682872295379639 tensor([0, 1, 1, 1, 1, 1, 1, 1, 2, 2], device='cuda:0') tensor([0, 4, 7, 6, 3, 9, 5, 2, 1, 8], device='cuda:0')\n","epoch 50: batch: 20/20 dist: 399 loss: 1.9493101835250854 tensor([0, 1, 1, 1, 1, 1, 1, 6, 6, 6], device='cuda:0') tensor([0, 5, 2, 7, 4, 9, 6, 1, 3, 8], device='cuda:0')\n","epoch 52: batch: 20/20 dist: 399 loss: 1.953015923500061 tensor([0, 1, 2, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 1, 6, 4, 9, 7, 8, 2, 5, 3], device='cuda:0')\n","epoch 54: batch: 20/20 dist: 392 loss: 1.9723262786865234 tensor([0, 2, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 4, 6, 3, 9, 7, 1, 8, 2, 5], device='cuda:0')\n","epoch 56: batch: 20/20 dist: 397 loss: 1.9621772766113281 tensor([0, 3, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 5, 6, 7, 8, 1, 3, 4, 9, 2], device='cuda:0')\n","epoch 58: batch: 20/20 dist: 392 loss: 1.9713473320007324 tensor([0, 1, 2, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 3, 6, 4, 8, 5, 7, 9, 2, 1], device='cuda:0')\n","epoch 60: batch: 20/20 dist: 395 loss: 1.9690968990325928 tensor([0, 1, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([0, 8, 5, 4, 6, 9, 1, 2, 7, 3], device='cuda:0')\n","epoch 62: batch: 20/20 dist: 393 loss: 1.9523977041244507 tensor([0, 2, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 3, 9, 2, 7, 1, 5, 8, 4, 6], device='cuda:0')\n","epoch 64: batch: 20/20 dist: 392 loss: 1.937119722366333 tensor([0, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 1, 7, 3, 2, 4, 5, 6, 8, 9], device='cuda:0')\n","epoch 66: batch: 20/20 dist: 385 loss: 1.9547078609466553 tensor([0, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([0, 1, 8, 5, 2, 4, 9, 6, 7, 3], device='cuda:0')\n","epoch 68: batch: 20/20 dist: 394 loss: 1.957369327545166 tensor([0, 7, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([0, 3, 8, 6, 2, 4, 7, 1, 9, 5], device='cuda:0')\n","epoch 70: batch: 20/20 dist: 390 loss: 1.9357922077178955 tensor([0, 2, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 1, 8, 5, 4, 6, 9, 3, 7, 2], device='cuda:0')\n","epoch 72: batch: 20/20 dist: 391 loss: 1.9350489377975464 tensor([0, 2, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 1, 3, 5, 6, 4, 8, 7, 2, 9], device='cuda:0')\n","epoch 74: batch: 20/20 dist: 392 loss: 1.9511594772338867 tensor([0, 2, 1, 1, 1, 1, 1, 1, 2, 2], device='cuda:0') tensor([0, 7, 6, 3, 8, 2, 1, 4, 9, 5], device='cuda:0')\n","epoch 76: batch: 20/20 dist: 389 loss: 1.9373947381973267 tensor([0, 2, 2, 2, 2, 2, 5, 5, 5, 5], device='cuda:0') tensor([0, 2, 9, 1, 5, 4, 3, 7, 6, 8], device='cuda:0')\n","epoch 78: batch: 20/20 dist: 397 loss: 1.976231336593628 tensor([0, 1, 6, 6, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 5, 8, 1, 9, 6, 7, 4, 3], device='cuda:0')\n","epoch 80: batch: 20/20 dist: 391 loss: 1.9251030683517456 tensor([0, 6, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 8, 5, 4, 3, 6, 1, 7, 9, 2], device='cuda:0')\n","epoch 82: batch: 20/20 dist: 389 loss: 1.9009873867034912 tensor([0, 1, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 5, 7, 6, 9, 4, 3, 2, 8, 1], device='cuda:0')\n","epoch 84: batch: 20/20 dist: 398 loss: 1.8943555355072021 tensor([0, 5, 1, 1, 7, 7, 5, 5, 5, 5], device='cuda:0') tensor([0, 2, 4, 6, 3, 1, 7, 5, 9, 8], device='cuda:0')\n","epoch 86: batch: 20/20 dist: 391 loss: 1.9062321186065674 tensor([0, 1, 5, 5, 2, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 6, 3, 8, 2, 5, 4, 9, 1, 7], device='cuda:0')\n","epoch 88: batch: 20/20 dist: 385 loss: 1.8816851377487183 tensor([0, 6, 1, 1, 1, 1, 1, 2, 2, 2], device='cuda:0') tensor([0, 4, 5, 8, 9, 3, 1, 6, 2, 7], device='cuda:0')\n","epoch 90: batch: 20/20 dist: 391 loss: 1.8899919986724854 tensor([0, 4, 1, 1, 1, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 2, 6, 8, 1, 3, 4, 9, 5, 7], device='cuda:0')\n","epoch 92: batch: 20/20 dist: 392 loss: 1.8878319263458252 tensor([0, 5, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([0, 2, 5, 7, 1, 8, 6, 3, 4, 9], device='cuda:0')\n","epoch 94: batch: 20/20 dist: 390 loss: 1.8602240085601807 tensor([0, 4, 1, 1, 1, 1, 1, 2, 2, 2], device='cuda:0') tensor([0, 7, 9, 1, 2, 4, 8, 3, 6, 5], device='cuda:0')\n","epoch 96: batch: 20/20 dist: 386 loss: 1.8097504377365112 tensor([0, 4, 5, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([0, 9, 6, 8, 2, 5, 4, 7, 1, 3], device='cuda:0')\n","epoch 98: batch: 20/20 dist: 396 loss: 1.8766802549362183 tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0, 8, 1, 7, 3, 9, 5, 4, 2, 6], device='cuda:0')\n","epoch 100: batch: 20/20 dist: 397 loss: 1.8216497898101807 tensor([0, 1, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 1, 4, 7, 9, 5, 8, 2, 6, 3], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LdEboubnXXoj","colab_type":"text"},"source":["# LSTM Training"]},{"cell_type":"code","metadata":{"id":"vBs89uoNXPc_","colab_type":"code","outputId":"304f7c5c-8b63-44b6-a911-cfba13a037db","executionInfo":{"status":"ok","timestamp":1556853747128,"user_tz":240,"elapsed":155357,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":884}},"source":["batch_size = 50\n","dataloader = DataLoader(tspdataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","hidden_size = 256\n","\n","pointer_net = Pointer_LSTM(hidden_size, batch_size)\n","\n","if torch.cuda.is_available():\n","    USE_CUDA = True\n","    print('Using GPU, %i devices.' % torch.cuda.device_count())\n","    pointer_net.cuda()\n","    net = torch.nn.DataParallel(pointer_net, device_ids=range(torch.cuda.device_count()))\n","    cudnn.benchmark = True\n","else:\n","    print(\"Not using GPU\")\n","    USE_CUDA = False\n","    \n","lr = 1e-3\n","    \n","CCE = torch.nn.CrossEntropyLoss()\n","pointer_optim = optim.RMSprop(filter(lambda p: p.requires_grad,\n","                            pointer_net.parameters()),\n","                     lr=lr)\n","    \n","def train(epochs=10, step=1000, name=\"model\"):\n","    losses = []\n","    batch_accs = []\n","    eps = []\n","    Dist = []\n","    \n","    for epoch in range(epochs):\n","        for j, (input_tensor, target_tensor) in enumerate(dataloader):\n","           \n","            if USE_CUDA:\n","                input_tensor = input_tensor.cuda()\n","                target_tensor = target_tensor.cuda()\n","\n","            o, p = pointer_net(input_tensor)\n","            loss = CCE(o, target_tensor)\n","                        \n","            if ((len(dataloader))*(epoch) + (j+1)) % step == 0:\n","                \n","                compare = target_tensor == p\n","                acc_sum = compare.sum(dim=1)\n","                correct = 0.\n","                dist = compare[compare == 0].size()[0]\n","                for i in range(batch_size):\n","                    if acc_sum[i].item() == target_tensor.size(1):\n","                        correct += 1\n","                acc = correct / batch_size * 100\n","                batch_accs.append(acc)\n","                losses.append(loss.item())\n","                eps.append((epoch+1))\n","                Dist.append(dist)\n","                print(\"epoch {}: batch: {}/{} dist: {} loss: {}\".format(epoch+1,j+1,len(dataloader),dist,loss.item()), p[0], target_tensor[0])\n","                to_save = {\"pointer_net_state_dict\" : pointer_net.state_dict(), \"pointer_optim_state_dict\" : pointer_optim.state_dict(), \"epoch\" : epoch, \"j\" : j}\n","                torch.save(to_save, join(path, \"pointer_net_\"+name+\"_\"+str(epoch)+\".pt\"))\n","            \n","            pointer_optim.zero_grad()\n","            loss.backward()\n","#             torch.nn.utils.clip_grad_norm_(pointer_net.parameters(), 0.5)\n","            pointer_optim.step()\n","            \n","#     for l, a in zip(losses, batch_accs):\n","#         print(l,a)\n","\n","    return losses, eps, Dist\n","\n","losses_lstm, epochs_lstm, dist_lstm = train(100, step=20*2, name=\"lstm_unmasked\")\n","            "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using GPU, 1 devices.\n","epoch 2: batch: 20/20 dist: 398 loss: 2.208016872406006 tensor([0, 2, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 5, 9, 3, 4, 8, 1, 7, 6], device='cuda:0')\n","epoch 4: batch: 20/20 dist: 398 loss: 2.2093985080718994 tensor([0, 1, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 3, 9, 8, 4, 1, 7, 2, 5, 6], device='cuda:0')\n","epoch 6: batch: 20/20 dist: 395 loss: 2.203101873397827 tensor([0, 6, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 2, 4, 3, 6, 5, 1, 9, 8, 7], device='cuda:0')\n","epoch 8: batch: 20/20 dist: 397 loss: 2.201049566268921 tensor([0, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 7, 9, 2, 4, 6, 3, 1, 8, 5], device='cuda:0')\n","epoch 10: batch: 20/20 dist: 391 loss: 2.1994447708129883 tensor([0, 1, 2, 3, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 1, 2, 3, 7, 6, 5, 4, 8, 9], device='cuda:0')\n","epoch 12: batch: 20/20 dist: 394 loss: 2.1961591243743896 tensor([0, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 2, 4, 3, 1, 9, 7, 8, 6], device='cuda:0')\n","epoch 14: batch: 20/20 dist: 390 loss: 2.1952807903289795 tensor([0, 1, 1, 7, 7, 7, 8, 8, 8, 8], device='cuda:0') tensor([0, 4, 7, 8, 2, 5, 3, 9, 1, 6], device='cuda:0')\n","epoch 16: batch: 20/20 dist: 391 loss: 2.203221082687378 tensor([0, 1, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 1, 5, 6, 7, 3, 2, 8, 9, 4], device='cuda:0')\n","epoch 18: batch: 20/20 dist: 396 loss: 2.201063394546509 tensor([0, 2, 8, 8, 8, 8, 9, 9, 9, 9], device='cuda:0') tensor([0, 4, 6, 1, 3, 2, 5, 9, 7, 8], device='cuda:0')\n","epoch 20: batch: 20/20 dist: 376 loss: 2.1902966499328613 tensor([0, 5, 8, 8, 8, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 3, 8, 7, 6, 5, 1, 4, 9], device='cuda:0')\n","epoch 22: batch: 20/20 dist: 385 loss: 2.187417984008789 tensor([0, 9, 1, 1, 1, 1, 1, 9, 9, 9], device='cuda:0') tensor([0, 5, 1, 2, 7, 6, 3, 4, 8, 9], device='cuda:0')\n","epoch 24: batch: 20/20 dist: 390 loss: 2.1884548664093018 tensor([0, 1, 3, 3, 3, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 1, 6, 5, 9, 7, 3, 2, 8, 4], device='cuda:0')\n","epoch 26: batch: 20/20 dist: 395 loss: 2.190258264541626 tensor([0, 2, 2, 2, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 9, 4, 8, 3, 2, 7, 1, 5, 6], device='cuda:0')\n","epoch 28: batch: 20/20 dist: 394 loss: 2.2054080963134766 tensor([0, 8, 2, 3, 3, 3, 3, 9, 9, 9], device='cuda:0') tensor([0, 8, 4, 5, 7, 1, 6, 3, 9, 2], device='cuda:0')\n","epoch 30: batch: 20/20 dist: 374 loss: 2.1779046058654785 tensor([0, 1, 6, 7, 8, 7, 8, 9, 9, 9], device='cuda:0') tensor([0, 2, 1, 9, 3, 4, 6, 8, 5, 7], device='cuda:0')\n","epoch 32: batch: 20/20 dist: 382 loss: 2.1628918647766113 tensor([0, 2, 2, 3, 3, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 8, 1, 9, 3, 6, 2, 5, 7, 4], device='cuda:0')\n","epoch 34: batch: 20/20 dist: 378 loss: 2.1750593185424805 tensor([0, 3, 1, 1, 1, 9, 9, 9, 8, 8], device='cuda:0') tensor([0, 3, 8, 1, 9, 4, 7, 6, 5, 2], device='cuda:0')\n","epoch 36: batch: 20/20 dist: 370 loss: 2.16290545463562 tensor([0, 6, 1, 1, 1, 1, 1, 1, 9, 9], device='cuda:0') tensor([0, 2, 5, 7, 9, 8, 1, 3, 6, 4], device='cuda:0')\n","epoch 38: batch: 20/20 dist: 371 loss: 2.152980327606201 tensor([0, 8, 4, 4, 4, 4, 5, 5, 7, 9], device='cuda:0') tensor([0, 1, 3, 6, 5, 8, 7, 4, 9, 2], device='cuda:0')\n","epoch 40: batch: 20/20 dist: 373 loss: 2.160207748413086 tensor([0, 1, 8, 1, 8, 9, 9, 9, 9, 5], device='cuda:0') tensor([0, 1, 3, 8, 9, 7, 6, 4, 5, 2], device='cuda:0')\n","epoch 42: batch: 20/20 dist: 362 loss: 2.141062021255493 tensor([0, 1, 8, 4, 8, 8, 8, 8, 8, 9], device='cuda:0') tensor([0, 1, 4, 7, 9, 5, 8, 2, 6, 3], device='cuda:0')\n","epoch 44: batch: 20/20 dist: 373 loss: 2.133802890777588 tensor([0, 7, 9, 2, 5, 6, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 5, 7, 1, 8, 6, 3, 4, 9], device='cuda:0')\n","epoch 46: batch: 20/20 dist: 364 loss: 2.1420037746429443 tensor([0, 6, 2, 2, 8, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 3, 7, 1, 9, 5, 4, 8, 6], device='cuda:0')\n","epoch 48: batch: 20/20 dist: 372 loss: 2.137657642364502 tensor([0, 4, 7, 7, 7, 7, 7, 7, 7, 9], device='cuda:0') tensor([0, 4, 2, 5, 8, 6, 9, 7, 3, 1], device='cuda:0')\n","epoch 50: batch: 20/20 dist: 372 loss: 2.135255813598633 tensor([0, 1, 1, 1, 7, 7, 7, 7, 9, 9], device='cuda:0') tensor([0, 2, 5, 4, 6, 3, 7, 9, 8, 1], device='cuda:0')\n","epoch 52: batch: 20/20 dist: 376 loss: 2.1489710807800293 tensor([0, 1, 8, 8, 8, 8, 8, 8, 8, 9], device='cuda:0') tensor([0, 9, 4, 5, 8, 7, 3, 6, 2, 1], device='cuda:0')\n","epoch 54: batch: 20/20 dist: 368 loss: 2.1238455772399902 tensor([0, 6, 1, 1, 1, 8, 8, 8, 8, 9], device='cuda:0') tensor([0, 4, 7, 8, 1, 3, 6, 5, 2, 9], device='cuda:0')\n","epoch 56: batch: 20/20 dist: 366 loss: 2.128415822982788 tensor([0, 4, 1, 1, 1, 7, 7, 7, 7, 9], device='cuda:0') tensor([0, 3, 6, 8, 7, 1, 2, 9, 4, 5], device='cuda:0')\n","epoch 58: batch: 20/20 dist: 367 loss: 2.114550828933716 tensor([0, 5, 6, 6, 6, 6, 6, 6, 8, 8], device='cuda:0') tensor([0, 7, 5, 3, 1, 6, 4, 2, 9, 8], device='cuda:0')\n","epoch 60: batch: 20/20 dist: 354 loss: 2.1143672466278076 tensor([0, 1, 3, 3, 4, 4, 4, 4, 9, 9], device='cuda:0') tensor([0, 8, 7, 3, 4, 6, 2, 5, 1, 9], device='cuda:0')\n","epoch 62: batch: 20/20 dist: 363 loss: 2.119030714035034 tensor([0, 1, 3, 3, 3, 3, 9, 9, 6, 7], device='cuda:0') tensor([0, 1, 6, 4, 3, 5, 2, 9, 8, 7], device='cuda:0')\n","epoch 64: batch: 20/20 dist: 358 loss: 2.095285415649414 tensor([0, 4, 1, 1, 1, 9, 8, 8, 8, 5], device='cuda:0') tensor([0, 2, 8, 6, 1, 3, 5, 4, 7, 9], device='cuda:0')\n","epoch 66: batch: 20/20 dist: 373 loss: 2.1288270950317383 tensor([0, 7, 9, 9, 9, 9, 9, 9, 5, 6], device='cuda:0') tensor([0, 6, 2, 4, 5, 7, 8, 1, 9, 3], device='cuda:0')\n","epoch 68: batch: 20/20 dist: 362 loss: 2.0824029445648193 tensor([0, 4, 2, 1, 1, 6, 6, 6, 9, 9], device='cuda:0') tensor([0, 3, 2, 8, 7, 1, 6, 4, 5, 9], device='cuda:0')\n","epoch 70: batch: 20/20 dist: 373 loss: 2.088428497314453 tensor([0, 3, 3, 3, 3, 4, 4, 8, 7, 7], device='cuda:0') tensor([0, 8, 5, 4, 6, 9, 1, 2, 7, 3], device='cuda:0')\n","epoch 72: batch: 20/20 dist: 359 loss: 2.0830891132354736 tensor([0, 7, 1, 1, 1, 9, 9, 9, 8, 7], device='cuda:0') tensor([0, 9, 6, 1, 4, 2, 7, 3, 8, 5], device='cuda:0')\n","epoch 74: batch: 20/20 dist: 358 loss: 2.0621564388275146 tensor([0, 1, 5, 3, 7, 8, 8, 8, 8, 9], device='cuda:0') tensor([0, 1, 8, 7, 6, 3, 9, 4, 5, 2], device='cuda:0')\n","epoch 76: batch: 20/20 dist: 360 loss: 2.078176259994507 tensor([0, 6, 9, 1, 1, 9, 9, 9, 7, 7], device='cuda:0') tensor([0, 6, 5, 8, 7, 1, 3, 9, 4, 2], device='cuda:0')\n","epoch 78: batch: 20/20 dist: 365 loss: 2.0597634315490723 tensor([0, 1, 5, 3, 8, 8, 8, 8, 8, 3], device='cuda:0') tensor([0, 1, 5, 2, 7, 8, 9, 4, 6, 3], device='cuda:0')\n","epoch 80: batch: 20/20 dist: 363 loss: 2.050870895385742 tensor([0, 4, 3, 3, 3, 3, 8, 7, 6, 6], device='cuda:0') tensor([0, 6, 8, 2, 1, 3, 5, 7, 4, 9], device='cuda:0')\n","epoch 82: batch: 20/20 dist: 371 loss: 2.068687915802002 tensor([0, 1, 9, 5, 5, 8, 8, 5, 5, 5], device='cuda:0') tensor([0, 1, 5, 4, 8, 3, 7, 6, 9, 2], device='cuda:0')\n","epoch 84: batch: 20/20 dist: 350 loss: 2.0495901107788086 tensor([0, 6, 1, 1, 1, 1, 1, 1, 3, 8], device='cuda:0') tensor([0, 8, 2, 9, 6, 5, 1, 3, 7, 4], device='cuda:0')\n","epoch 86: batch: 20/20 dist: 363 loss: 2.03933048248291 tensor([0, 5, 5, 4, 8, 8, 8, 9, 9, 9], device='cuda:0') tensor([0, 5, 7, 8, 3, 2, 1, 6, 4, 9], device='cuda:0')\n","epoch 88: batch: 20/20 dist: 348 loss: 2.0275015830993652 tensor([0, 7, 4, 8, 8, 8, 8, 8, 4, 9], device='cuda:0') tensor([0, 6, 7, 4, 8, 2, 5, 1, 3, 9], device='cuda:0')\n","epoch 90: batch: 20/20 dist: 350 loss: 2.034291982650757 tensor([0, 1, 2, 8, 8, 8, 7, 7, 6, 6], device='cuda:0') tensor([0, 4, 2, 7, 5, 9, 8, 6, 3, 1], device='cuda:0')\n","epoch 92: batch: 20/20 dist: 355 loss: 2.0318868160247803 tensor([0, 3, 8, 1, 9, 8, 8, 9, 9, 7], device='cuda:0') tensor([0, 8, 4, 6, 2, 3, 1, 9, 5, 7], device='cuda:0')\n","epoch 94: batch: 20/20 dist: 365 loss: 2.040928363800049 tensor([0, 5, 5, 1, 8, 8, 5, 6, 6, 9], device='cuda:0') tensor([0, 3, 8, 7, 6, 2, 1, 9, 4, 5], device='cuda:0')\n","epoch 96: batch: 20/20 dist: 370 loss: 2.071995258331299 tensor([0, 3, 6, 1, 8, 8, 8, 7, 7, 9], device='cuda:0') tensor([0, 3, 2, 9, 4, 1, 6, 7, 5, 8], device='cuda:0')\n","epoch 98: batch: 20/20 dist: 361 loss: 2.011840343475342 tensor([0, 4, 2, 3, 3, 5, 5, 8, 8, 8], device='cuda:0') tensor([0, 6, 2, 3, 9, 5, 1, 7, 4, 8], device='cuda:0')\n","epoch 100: batch: 20/20 dist: 337 loss: 2.004096031188965 tensor([0, 4, 2, 3, 3, 5, 5, 5, 8, 8], device='cuda:0') tensor([0, 6, 2, 3, 9, 5, 1, 7, 4, 8], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UFLmpGmGaj2z","colab_type":"code","colab":{}},"source":["torch.save(pointer_net.state_dict(), join(path,\"masked_pointernet_final.pt\"))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFRw6CZAaDu2","colab_type":"code","outputId":"9840b74b-9e01-4ac9-d983-429c643c1ed9","executionInfo":{"status":"ok","timestamp":1556808166751,"user_tz":240,"elapsed":1404,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["total_missed = 0.\n","total_dist = 0.\n","\n","opt_total = 0.\n","\n","dataloader = DataLoader(tspdataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","for j, (input_tensor, target_tensor) in enumerate(dataloader):\n","           \n","    if USE_CUDA:\n","        input_tensor = input_tensor.cuda()\n","        target_tensor = target_tensor.cuda()\n","\n","    o, p = pointer_net(input_tensor)\n","    \n","    print(input_tensor.size(), target_tensor.size())\n","\n","    compare = target_tensor == p\n","    acc_sum = compare.sum(dim=1)\n","    dist = compare[compare == 0].size()[0]\n","    total_missed += dist\n","    \n","    tour_dist = 0.\n","    opt_dist = 0.\n","    \n","    for k in range(50):\n","#         print(input_tensor[k,])\n","        for i in range(19):\n","\n","#             print(input_tensor[k,p[k,i],:], input_tensor[k,p[k,i+1],:])\n","#             print(p[k,i])\n","            x = torch.pow(input_tensor[k,p[k,i],0] - input_tensor[k,p[k,i+1],0],2)\n","            y = torch.pow(input_tensor[k,p[k,i],1] - input_tensor[k,p[k,i+1],1],2)\n","            td = torch.sqrt(x + y)\n","            \n","            x_opt = torch.pow(input_tensor[k,target_tensor[k,i],0] - input_tensor[k,target_tensor[k,i+1],0],2)\n","            y_opt = torch.pow(input_tensor[k,target_tensor[k,i],1] - input_tensor[k,target_tensor[k,i+1],1],2)\n","            td_opt = torch.sqrt(x_opt + y_opt)\n","#             print(td)\n","\n","            tour_dist += td\n","            opt_dist += td_opt\n","        \n","    total_dist += tour_dist\n","    opt_total += opt_dist\n","    \n","print(total_dist / (50. * len(dataloader)))\n","print(opt_total / (50. * len(dataloader)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([50, 20, 2]) torch.Size([50, 20])\n","torch.Size([50, 20, 2]) torch.Size([50, 20])\n","tensor(8.6708, device='cuda:0')\n","tensor(4.0439, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EulSVbG-Ftph","colab_type":"code","outputId":"5049a62f-7f38-46ee-9644-22daedd26c24","executionInfo":{"status":"ok","timestamp":1556853834216,"user_tz":240,"elapsed":370,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["X = [40, 80, 120, 160, 200, 240, 280, 320, 360, 400, 440, 480, 520, 560, 600, 640, 680, 720, 760, 800, 840, 880, 920, 960, 1000]\n","Y = [2.1689043045043945, 2.049980878829956, 1.8807538747787476, 1.8862016201019287, 1.872876763343811, 1.8428858518600464, 1.7948155403137207, 1.7852349281311035, 1.8030064105987549, 1.7452983856201172, 1.761432409286499, 1.7307560443878174, 1.755123257637024, 1.7456105947494507, 1.7584829330444336, 1.7238335609436035, 1.7694389820098877, 1.6829913854599, 1.7074567079544067, 1.731794834136963, 1.6763498783111572, 1.7016667127609253, 1.647472620010376, 1.684429407119751, 1.6404321193695068]\n","\n","import matplotlib.pyplot as plt\n","\n","plt.ylabel('Mistakes')\n","plt.xlabel('Epochs')\n","plt.title('LSTM Mistakes Over Time')\n","plt.plot(epochs_lstm,dist_lstm)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f846010fcc0>]"]},"metadata":{"tags":[]},"execution_count":30},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}