{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of pointer_net_tsp_prototype.ipynb","version":"0.3.2","provenance":[{"file_id":"17Pvt8qVqw04bUfhNb6Z64sklVOGSo8jv","timestamp":1556817900263},{"file_id":"1kAfQvu-B_aUHdUMF9AOqL-TiHcIZISoo","timestamp":1555859258409},{"file_id":"1O1ZkUb5S_IzHePHQEUuTa0qkjFmTrZtI","timestamp":1555449701261},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/seq2seq_translation_tutorial.ipynb","timestamp":1555257061838}],"collapsed_sections":["bTfLcxSCc2Jr"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Hiwi182oc9qS","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"IG21wvnzknsT","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","from os.path import join\n","\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NfCHYJNcgLw","colab_type":"text"},"source":["# Data processing"]},{"cell_type":"code","metadata":{"id":"AE7XxbKoYCKc","colab_type":"code","outputId":"61c772ad-c4b9-4054-a4ef-96af08257f93","executionInfo":{"status":"ok","timestamp":1556828791319,"user_tz":240,"elapsed":23785,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","path = \"/content/gdrive/My Drive/Colab Notebooks/\"\n","drive.mount('/content/gdrive')\n","!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/tsp_20_test.txt ."],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yTbgo9nSCOUo","colab_type":"code","outputId":"55872c53-5c58-41f7-eb10-e070feea027a","executionInfo":{"status":"ok","timestamp":1556828813561,"user_tz":240,"elapsed":18801,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# copied from https://github.com/keon/pointer-networks/blob/master/tsp_data.py\n","\n","import math\n","import numpy as np\n","import random\n","import itertools\n","from sklearn.model_selection import train_test_split\n","\n","class Tsp:\n","    def next_batch(self, batch_size=1):\n","        X, Y = [], []\n","        for b in range(batch_size):\n","#             print(\"preparing dataset... %s/%s\" % (b, batch_size))\n","            points = self.generate_data()\n","            solved = self.solve_tsp_dynamic(points)\n","            X.append(points), Y.append(solved)\n","        return np.asarray(X), np.asarray(Y)\n","\n","    def length(self, x, y):\n","        return (math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2))\n","\n","    def solve_tsp_dynamic(self, points):\n","        # calc all lengths\n","        all_distances = [[self.length(x, y) for y in points] for x in points]\n","        # initial value - just distance from 0 to\n","        # every other point + keep the track of edges\n","        A = {(frozenset([0, idx+1]), idx+1): (dist, [0, idx+1])\n","             for idx, dist in enumerate(all_distances[0][1:])}\n","        cnt = len(points)\n","        for m in range(2, cnt):\n","            B = {}\n","            for S in [frozenset(C) | {0}\n","                      for C in itertools.combinations(range(1, cnt), m)]:\n","                for j in S - {0}:\n","                    B[(S, j)] = min([(A[(S-{j}, k)][0] + all_distances[k][j],\n","                                      A[(S-{j}, k)][1] + [j])\n","                                     for k in S if k != 0 and k != j])\n","            A = B\n","        res = min([(A[d][0] + all_distances[0][d[1]], A[d][1])\n","                   for d in iter(A)])\n","        return res[1]\n","\n","    def generate_data(self, N=10):\n","        radius = 1\n","        rangeX = (0, 10)\n","        rangeY = (0, 10)\n","        qty = N\n","\n","        deltas = set()\n","        for x in range(-radius, radius+1):\n","            for y in range(-radius, radius+1):\n","                if x*x + y*y <= radius*radius:\n","                    deltas.add((x, y))\n","\n","        randPoints = []\n","        excluded = set()\n","        i = 0\n","        while i < qty:\n","            x = random.randrange(*rangeX)\n","            y = random.randrange(*rangeY)\n","            if (x, y) in excluded:\n","                continue\n","            randPoints.append((x, y))\n","            i += 1\n","            excluded.update((x+dx, y+dy) for (dx, dy) in deltas)\n","        return randPoints\n","    \n","def read_from_file():\n","    with open(\"tsp_20_test.txt\") as file:\n","        X = []\n","        Y = []\n","\n","        for line in file:\n","            x, y = line.strip().split(\" output \")\n","            x = x.strip().split()\n","            x = [float(i) for i in x]\n","            x = [[x[i], x[i+1]] for i in range(0,len(x)-1,2)]\n","            y = y.strip().split()\n","            y = [int(i)-1 for i in y[:-1]]\n","            X.append(x)\n","            Y.append(y)\n","        X = np.array(X)\n","        Y = np.array(Y)\n","        return X, Y\n","    \n"," \n","    \n","p = Tsp()\n","X, Y = p.next_batch(1000)\n","# X, Y = read_from_file()\n","# X, Y = X[:100], Y[:100]\n","print(X.shape, Y.shape)\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(1000, 10, 2) (1000, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D265jt86rMYw","colab_type":"code","colab":{}},"source":["class TSPDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.Y = Y\n","        self.data_size = X.shape[0]\n","        self.label_size = Y.shape[1]\n","        \n","    def __len__(self):\n","        return self.data_size\n","    def __getitem__(self, idx):\n","        input_tensor = torch.from_numpy(self.X[idx]).float()   \n","        target_tensor = torch.from_numpy(self.Y[idx]).long()\n","\n","        return input_tensor, target_tensor\n","\n","tspdataset = TSPDataset(X, Y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyTajeq4cyuc","colab_type":"text"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"9R5smuAEkEeC","colab_type":"code","colab":{}},"source":["class Encoder_GRU(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Encoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.batch_size = batch_size\n","        self.relu = nn.ReLU()\n","        \n","        self.embedding = nn.Linear(2, self.hidden_size)\n","        self.rnn = nn.GRU(self.hidden_size, self.hidden_size)\n","        \n","    def forward(self, input, hidden):\n","        embedded = self.relu(self.embedding(input)).permute(1, 0, 2)\n","        output, hidden = self.rnn(embedded, hidden)\n","        return output.permute(1,0,2) , hidden, embedded.permute(1,0,2)\n","    \n","    def init_hidden(self):\n","        return torch.zeros(1,self.batch_size,self.hidden_size)\n","\n","class Encoder_LSTM(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Encoder_LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.batch_size = batch_size\n","        self.relu = nn.ReLU()\n","        self.n_layers = 1\n","        \n","        self.embedding = nn.Linear(2, self.hidden_size)\n","        self.rnn = nn.LSTM(self.hidden_size, self.hidden_size, self.n_layers)\n","        \n","        self.c_x = nn.Parameter(torch.zeros(1), requires_grad=False)\n","        self.h_x = nn.Parameter(torch.zeros(1), requires_grad=False)\n","        \n","    def forward(self, input, hidden, context):\n","        embedded = self.relu(self.embedding(input)).permute(1, 0, 2)\n","        output, (hidden, context) = self.rnn(embedded, (hidden, context))\n","        return output.permute(1,0,2) , hidden, embedded.permute(1,0,2), context\n","    \n","    def init_hidden(self):\n","        c_x = self.c_x.unsqueeze(0).unsqueeze(0).repeat(self.n_layers, self.batch_size, self.hidden_size)\n","        h_x = self.h_x.unsqueeze(0).unsqueeze(0).repeat(self.n_layers, self.batch_size, self.hidden_size)\n","\n","        return h_x, c_x\n","\n","class Attention_GRU(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention_GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        self.W_hidden = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_encoded = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.V = nn.Parameter(torch.FloatTensor(self.hidden_size), requires_grad=True)\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        nn.init.uniform_(self.V, -1, 1)\n","    \n","    def forward(self, input, hidden):\n","        encoder_outputs = self.W_encoded(input).permute(1,0,2)\n","        hidden = self.W_hidden(hidden)\n","        summed = self.tanh(encoder_outputs + hidden).permute(0,2,1)\n","        V = self.V.unsqueeze(0).expand(summed.size(0), -1).unsqueeze(1)\n","        attn_weights = torch.bmm(V, summed).squeeze().permute(1,0)\n","        alpha = self.softmax(attn_weights).unsqueeze(1)\n","        ct = torch.bmm(alpha, input).squeeze()\n","        return alpha.squeeze(), ct, attn_weights\n","\n","class Attention_LSTM(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention_LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.n_layers = 1\n","        \n","        self.W_encoded = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_hidden_context = nn.Linear(self.hidden_size * 1, self.hidden_size)\n","        self.W_average = nn.Linear(self.n_layers, 1)\n","        self.V = nn.Parameter(torch.FloatTensor(self.hidden_size), requires_grad=True)\n","        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        nn.init.uniform_(self.V, -1, 1)\n","    \n","    def forward(self, input, hidden, context, mask):\n","        encoder_outputs = self.W_encoded(input).permute(1,0,2)\n","#         hidden_context = torch.cat((hidden, context), dim=2)\n","        hidden_context = hidden\n","        hidden_context = self.W_hidden_context(hidden_context)\n","#         hidden_context = torch.mean(hidden_context, dim=0)\n","        hidden_context = hidden_context.permute(2,1,0)\n","        hidden_context = self.W_average(hidden_context).permute(2,1,0)\n","        summed = self.tanh(encoder_outputs + hidden_context).permute(0,2,1)\n","        V = self.V.unsqueeze(0).expand(summed.size(0), -1).unsqueeze(1)\n","        attn_weights = torch.bmm(V, summed).squeeze().permute(1,0)\n","        if (len(attn_weights[mask]) > 0):\n","            attn_weights[mask] = self.inf[mask]\n","        alpha = self.softmax(attn_weights).unsqueeze(1)\n","        ct = torch.bmm(alpha, input).squeeze()\n","        return alpha.squeeze(), ct, attn_weights\n","    \n","    def init_inf(self, size):\n","        self.inf = self._inf.unsqueeze(1).expand(*size)\n","\n","class Attention_LSTM_Beam(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention_LSTM_Beam, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.n_layers = 1\n","        \n","        self.W_encoded = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_hidden_context = nn.Linear(self.hidden_size * 1, self.hidden_size)\n","        self.W_average = nn.Linear(self.n_layers, 1)\n","        self.V = nn.Parameter(torch.FloatTensor(self.hidden_size), requires_grad=True)\n","        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        nn.init.uniform_(self.V, -1, 1)\n","    \n","    def forward(self, input, hidden, context):\n","        encoder_outputs = self.W_encoded(input).permute(1,0,2)\n","#         hidden_context = torch.cat((hidden, context), dim=2)\n","        hidden_context = hidden\n","        hidden_context = self.W_hidden_context(hidden_context)\n","#         hidden_context = torch.mean(hidden_context, dim=0)\n","        hidden_context = hidden_context.permute(2,1,0)\n","        hidden_context = self.W_average(hidden_context).permute(2,1,0)\n","        summed = self.tanh(encoder_outputs + hidden_context).permute(0,2,1)\n","        V = self.V.unsqueeze(0).expand(summed.size(0), -1).unsqueeze(1)\n","        attn_weights = torch.bmm(V, summed).squeeze().permute(1,0)\n","        alpha = self.softmax(attn_weights).unsqueeze(1)\n","        ct = torch.bmm(alpha, input).squeeze()\n","        return alpha.squeeze(), ct, attn_weights\n","        \n","    \n","class Decoder_GRU(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Decoder_GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        self.rnn = nn.GRU(self.hidden_size*2, self.hidden_size)\n","        self.attention = Attention_GRU(self.hidden_size)\n","        \n","    def forward(self, input, hidden, encoder_outputs, embedded):\n","        outputs = []\n","        pointers = []\n","                \n","        for _ in range(encoder_outputs.size(1)):\n","            alpha, ct, attn_weights = self.attention(encoder_outputs, hidden)\n","            w = torch.cat((input, ct),1).unsqueeze(0)\n","            _, hidden = self.rnn(w, hidden)\n","            _, topi = alpha.topk(1)\n","            topi = topi.squeeze().detach()\n","            input_ = []\n","            for j, i in enumerate(topi):\n","                input_.append(embedded[j,i,:].unsqueeze(0))\n","            input = torch.cat(input_)\n","            outputs.append(attn_weights)\n","            pointers.append(topi)\n","        \n","        return torch.stack(outputs,1), torch.stack(pointers,1)\n","        \n","class Decoder_LSTM(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Decoder_LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.n_layers = 1\n","        \n","        self.rnn = nn.LSTM(self.hidden_size*2, self.hidden_size, self.n_layers)\n","        self.attention = Attention_LSTM(self.hidden_size)\n","        \n","        self.mask = nn.Parameter(torch.ones(1), requires_grad=False)\n","        \n","    def forward(self, input, hidden, encoder_outputs, embedded, context):\n","        outputs = []\n","        pointers = []\n","        \n","        mask = self.mask.repeat(encoder_outputs.size(1)).unsqueeze(0).repeat(encoder_outputs.size(0), 1)\n","        self.attention.init_inf(mask.size())\n","                \n","        for _ in range(encoder_outputs.size(1)):\n","            alpha, ct, attn_weights = self.attention(encoder_outputs, hidden, context, torch.eq(mask, 0))\n","            w = torch.cat((input, ct),1).unsqueeze(0)\n","            _, (hidden, context) = self.rnn(w, (hidden, context))\n","            \n","            masked_alpha = alpha * mask\n","            \n","            _, topi = masked_alpha.max(1)\n","            input_ = []\n","            for j, i in enumerate(topi):\n","                input_.append(embedded[j,i,:].unsqueeze(0))\n","                mask[j,i] = 0.\n","            input = torch.cat(input_)\n","            outputs.append(alpha)\n","            pointers.append(topi)\n","        \n","        return torch.stack(outputs,1), torch.stack(pointers,1)  \n","\n","class Decoder_LSTM_Beam(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Decoder_LSTM_Beam, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.n_layers = 1\n","        \n","        self.rnn = nn.LSTM(self.hidden_size*2, self.hidden_size, self.n_layers)\n","        self.attention = Attention_LSTM_Beam(self.hidden_size)\n","        \n","        self.prev_dist = nn.Parameter(torch.zeros(1), requires_grad=False)\n","        self.curr_dist = nn.Parameter(torch.zeros(1), requires_grad=False)\n","                \n","    def forward(self, input, hidden, encoder_outputs, embedded, context, eval=False):\n","        \n","        if eval:\n","            \n","            dists = []\n","            \n","            batch_size = encoder_outputs.size(0)\n","            seq_len = encoder_outputs.size(1)\n","            prev_dist = self.prev_dist.repeat(seq_len).unsqueeze(0).repeat(batch_size, 1)\n","            \n","            hiddens = []\n","            contexts = []\n","            \n","            for i in range(seq_len):\n","                \n","                if i == 0:\n","                    alpha, ct, attn_weights = self.attention(encoder_outputs, hidden, context)\n","                    w = torch.cat((input, ct),1).unsqueeze(0)\n","                    _, (hidden, context) = self.rnn(w, (hidden, context))\n","                    \n","                    hiddens = [hidden] * seq_len\n","                    contexts = [context] * seq_len\n","                    \n","                    prev_dist = alpha\n","                    dists.append(prev_dist)\n","                    \n","                else:\n","                    curr_dist = self.curr_dist.repeat(seq_len).unsqueeze(0).repeat(batch_size, 1)\n","#                     curr_dist = torch.zeros(batch_size, seq_len)\n","                    for j in range(seq_len):\n","                        a = prev_dist[:,j].unsqueeze(1).unsqueeze(2)\n","                        \n","                        alpha, ct, attn_weights = self.attention(encoder_outputs, hiddens[j], contexts[j])\n","                        w = torch.cat((embedded[:,j,:], ct),1).unsqueeze(0)\n","                        _, (h, c) = self.rnn(w, (hiddens[j], contexts[j]))\n","                        hiddens[j] = h\n","                        contexts[j] = c\n","                        \n","                        b = alpha.unsqueeze(1)\n","                        curr_dist += torch.bmm(a,b).squeeze()\n","                \n","                    prev_dist = curr_dist\n","                    dists.append(prev_dist)\n","              \n","#             chosen = set()\n","            \n","#             outputs = []\n","#             pointers = []\n","\n","#             for d in dists:\n","#                 for c in chosen:\n","#                     for k in range(c.size(0)):\n","#                         d[k,c[k]] = -np.inf\n","#                 val, i = d.max(1)\n","#                 chosen.add(i)\n","#                 outputs.append(d)\n","#                 pointers.append(i)\n","\n","            chosen = set()\n","\n","            dists = torch.stack(dists)\n","\n","            pointers = []\n","\n","            for i in range(batch_size):\n","                dist = dists[:,i,:].squeeze()\n","                chosen = set()\n","                seq = [None] * seq_len\n","                for _ in range(seq_len):\n","                    j = torch.argmax(dist.flatten())\n","                    x_ = j % dist.size(0)\n","                    y_ = (j - x_) / dist.size(0)\n","                    seq[y_] = x_\n","                    dist[:,x_] = -np.inf\n","                    dist[y_,:] = -np.inf\n","\n","                pointers.append(seq)\n","            return None, torch.tensor(pointers)\n","\n","#             return torch.stack(outputs,1), torch.stack(pointers,1)              \n","            \n","        else:\n","\n","            outputs = []\n","            pointers = []\n","\n","            for _ in range(encoder_outputs.size(1)):\n","                alpha, ct, attn_weights = self.attention(encoder_outputs, hidden, context)\n","                w = torch.cat((input, ct),1).unsqueeze(0)\n","                _, (hidden, context) = self.rnn(w, (hidden, context))\n","\n","                _, topi = alpha.max(1)\n","                input_ = []\n","                for j, i in enumerate(topi):\n","                    input_.append(embedded[j,i,:].unsqueeze(0))\n","                input = torch.cat(input_)\n","                outputs.append(alpha)\n","                pointers.append(topi)\n","\n","            return torch.stack(outputs,1), torch.stack(pointers,1)  \n","    \n","class Pointer_LSTM(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Pointer_LSTM, self).__init__()\n","        self.encoder = Encoder_LSTM(hidden_size, batch_size)\n","        self.decoder = Decoder_LSTM(hidden_size)  \n","        self.decoder_input0 = nn.Parameter(torch.FloatTensor(batch_size, hidden_size), requires_grad=False)\n","        \n","        nn.init.uniform_(self.decoder_input0, -1, 1)\n","        \n","    def forward(self, input):\n","        eh0, ec0 = self.encoder.init_hidden()\n","        output, hidden, embedded, context = self.encoder(input, eh0, ec0)\n","        o, p = self.decoder(self.decoder_input0, hidden, output, embedded, context)\n","        return o, p\n","  \n","class Pointer_LSTM_Beam(nn.Module):\n","    def __init__(self, hidden_size, batch_size):\n","        super(Pointer_LSTM_Beam, self).__init__()\n","        self.encoder = Encoder_LSTM(hidden_size, batch_size)\n","        self.decoder = Decoder_LSTM_Beam(hidden_size)  \n","        self.decoder_input0 = nn.Parameter(torch.FloatTensor(batch_size, hidden_size), requires_grad=False)\n","        \n","        nn.init.uniform_(self.decoder_input0, -1, 1)\n","        \n","    def forward(self, input, eval=False):\n","        eh0, ec0 = self.encoder.init_hidden()\n","        output, hidden, embedded, context = self.encoder(input, eh0, ec0)\n","        o, p = self.decoder(self.decoder_input0, hidden, output, embedded, context, eval)\n","        return o, p\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTfLcxSCc2Jr","colab_type":"text"},"source":["# GRU Training\n"]},{"cell_type":"code","metadata":{"id":"L7EhrRaS8CUi","colab_type":"code","outputId":"3f5eb2bc-8839-4421-84a0-57dee2a02b8c","executionInfo":{"status":"ok","timestamp":1556291894752,"user_tz":240,"elapsed":133312,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":2754}},"source":["batch_size = 50\n","dataloader = DataLoader(tspdataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","hidden_size = 256\n","encoder = Encoder_GRU(hidden_size, batch_size)\n","decoder = Decoder_GRU(hidden_size)\n","\n","decoder_input0 = nn.Parameter(torch.FloatTensor(batch_size, hidden_size), requires_grad=False)\n","nn.init.uniform_(decoder_input0, -1, 1)\n","\n","if torch.cuda.is_available():\n","    USE_CUDA = True\n","    print('Using GPU, %i devices.' % torch.cuda.device_count())\n","    encoder.cuda()\n","    decoder.cuda()\n","    cudnn.benchmark = True\n","    decoder_input0 = decoder_input0.cuda()\n","else:\n","    print(\"Not using GPU\")\n","    USE_CUDA = False\n","    \n","\n","\n","def train(epochs=10, lr=1e-4, step=1000):\n","    losses = []\n","    batch_accs = []\n","    CCE = torch.nn.CrossEntropyLoss()\n","    enc_optim = optim.Adam(filter(lambda p: p.requires_grad,\n","                                encoder.parameters()),\n","                         lr=lr)\n","\n","    dec_optim = optim.Adam(filter(lambda p: p.requires_grad,\n","                                decoder.parameters()),\n","                         lr=lr)\n","    \n","    \n","    \n","    steps = 0\n","    for epoch in range(epochs):\n","        for input_tensor, target_tensor in dataloader:\n","            \n","            eh0 = encoder.init_hidden()      \n","            \n","            if USE_CUDA:\n","                input_tensor = input_tensor.cuda()\n","                target_tensor = target_tensor.cuda()\n","                eh0 = eh0.cuda()\n","            \n","            output, hidden, embedded = encoder(input_tensor, eh0)\n","\n","            o, p = decoder(decoder_input0, hidden, output, embedded)    \n","            loss = CCE(o, target_tensor)\n","                        \n","            if steps % step == 0:\n","            \n","                acc_sum = (target_tensor == p).sum(dim=1)\n","                correct = 0.\n","                for i in range(batch_size):\n","                    if acc_sum[i].item() == target_tensor.size(1):\n","                        correct += 1\n","                acc = correct / batch_size * 100\n","                batch_accs.append(acc)\n","                losses.append(loss.item())\n","                print(steps, acc, loss.item(), p[0], target_tensor[0])\n","            \n","            enc_optim.zero_grad()\n","            dec_optim.zero_grad()\n","            loss.backward()\n","#             torch.nn.utils.clip_grad_norm_(encoder.parameters(), 0.5)\n","#             torch.nn.utils.clip_grad_norm_(decoder_input0, 0.5)\n","#             torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n","            enc_optim.step()\n","            dec_optim.step()\n","            \n","            steps += batch_size\n","#     for l, a in zip(losses, batch_accs):\n","#         print(l,a)\n","\n","train(800, step=5000)\n","            "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using GPU, 1 devices.\n","0 0.0 2.918652057647705 tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 6, 5, 9, 2, 4, 1, 3, 7, 8], device='cuda:0')\n","5000 0.0 1.9950535297393799 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 3, 7, 1, 8, 5, 2, 9, 4, 6], device='cuda:0')\n","10000 0.0 1.9635348320007324 tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 7, 9, 5, 3, 1, 8, 6, 4], device='cuda:0')\n","15000 0.0 1.9722297191619873 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 8, 6, 2, 9, 3, 5, 1, 7, 4], device='cuda:0')\n","20000 0.0 1.9615187644958496 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 6, 3, 4, 9, 8, 7, 2, 5, 1], device='cuda:0')\n","25000 0.0 1.9549351930618286 tensor([4, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 5, 2, 4, 9, 7, 3, 6, 1, 8], device='cuda:0')\n","30000 0.0 1.968154788017273 tensor([1, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 1, 7, 4, 6, 9, 3, 2, 8, 5], device='cuda:0')\n","35000 0.0 1.9585504531860352 tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 9, 4, 3, 7, 6, 5, 8, 1, 2], device='cuda:0')\n","40000 0.0 1.9443219900131226 tensor([5, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 8, 7, 6, 2, 4, 1, 3, 9, 5], device='cuda:0')\n","45000 0.0 1.931301236152649 tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 3, 6, 8, 7, 9, 1, 5, 2, 4], device='cuda:0')\n","50000 0.0 1.94705069065094 tensor([2, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 7, 3, 9, 8, 6, 4, 2, 1], device='cuda:0')\n","55000 0.0 1.9251816272735596 tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 7, 3, 1, 8, 5, 4, 2, 6, 9], device='cuda:0')\n","60000 0.0 1.9546605348587036 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 7, 6, 9, 4, 2, 1, 8, 5, 3], device='cuda:0')\n","65000 0.0 1.904040813446045 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 3, 5, 6, 1, 4, 8, 7, 2, 9], device='cuda:0')\n","70000 0.0 1.9319689273834229 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 2, 4, 3, 9, 5, 6, 7, 1, 8], device='cuda:0')\n","75000 0.0 1.9154194593429565 tensor([7, 5, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 8, 1, 4, 5, 7, 2, 6, 3, 9], device='cuda:0')\n","80000 0.0 1.878793716430664 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 4, 8, 2, 6, 1, 3, 9, 7, 5], device='cuda:0')\n","85000 0.0 1.8982250690460205 tensor([1, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 1, 2, 5, 4, 7, 8, 3, 9, 6], device='cuda:0')\n","90000 0.0 1.869999885559082 tensor([9, 7, 7, 7, 9, 9, 9, 9, 9, 7], device='cuda:0') tensor([0, 3, 7, 9, 8, 6, 4, 1, 2, 5], device='cuda:0')\n","95000 0.0 1.863792061805725 tensor([8, 7, 7, 7, 5, 7, 7, 5, 8, 7], device='cuda:0') tensor([0, 6, 3, 4, 1, 7, 2, 5, 8, 9], device='cuda:0')\n","100000 0.0 1.8723500967025757 tensor([3, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 1, 4, 5, 6, 8, 2, 9, 3, 7], device='cuda:0')\n","105000 0.0 1.8382800817489624 tensor([5, 6, 6, 6, 6, 6, 6, 5, 5, 6], device='cuda:0') tensor([0, 1, 3, 9, 8, 4, 5, 2, 6, 7], device='cuda:0')\n","110000 0.0 1.836361289024353 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 6, 1, 4, 9, 7, 2, 3, 8], device='cuda:0')\n","115000 0.0 1.8194791078567505 tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 3, 7, 6, 9, 5, 2, 1, 4, 8], device='cuda:0')\n","120000 0.0 1.7751778364181519 tensor([2, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 3, 7, 5, 6, 8, 1, 4, 2, 9], device='cuda:0')\n","125000 0.0 1.7545347213745117 tensor([3, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 3, 7, 1, 9, 8, 2, 4, 5, 6], device='cuda:0')\n","130000 0.0 1.7752702236175537 tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 8, 6, 2, 3, 9, 5, 1, 4, 7], device='cuda:0')\n","135000 0.0 1.7234925031661987 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 2, 3, 8, 4, 1, 9, 7, 6], device='cuda:0')\n","140000 0.0 1.6998319625854492 tensor([1, 7, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 2, 4, 3, 5, 6, 9, 8, 1, 7], device='cuda:0')\n","145000 0.0 1.6683077812194824 tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 9, 5, 4, 3, 8, 6, 7, 2, 1], device='cuda:0')\n","150000 0.0 1.6275663375854492 tensor([4, 4, 4, 4, 4, 4, 4, 5, 5, 7], device='cuda:0') tensor([0, 4, 7, 3, 2, 1, 9, 8, 6, 5], device='cuda:0')\n","155000 0.0 1.609433889389038 tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 4, 8, 7, 6, 9, 2, 1, 3, 5], device='cuda:0')\n","160000 0.0 1.5805624723434448 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 6, 2, 9, 1, 4, 8, 5, 3, 7], device='cuda:0')\n","165000 0.0 1.5184978246688843 tensor([2, 2, 2, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 5, 1, 8, 6, 2, 9, 4, 3, 7], device='cuda:0')\n","170000 0.0 1.4809716939926147 tensor([2, 2, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 4, 3, 9, 6, 7, 5, 8, 2, 1], device='cuda:0')\n","175000 0.0 1.4562177658081055 tensor([1, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 7, 1, 4, 9, 5, 2, 6, 3, 8], device='cuda:0')\n","180000 0.0 1.3889602422714233 tensor([6, 6, 6, 6, 6, 6, 6, 6, 8, 8], device='cuda:0') tensor([0, 3, 7, 4, 5, 6, 1, 8, 2, 9], device='cuda:0')\n","185000 0.0 1.38551664352417 tensor([2, 2, 4, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([0, 5, 7, 1, 3, 2, 8, 6, 4, 9], device='cuda:0')\n","190000 0.0 1.3068007230758667 tensor([8, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 5, 6, 8, 3, 7, 4, 9, 1, 2], device='cuda:0')\n","195000 0.0 1.309319257736206 tensor([9, 9, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 4, 7, 6, 3, 2, 9, 8, 1, 5], device='cuda:0')\n","200000 0.0 1.270499348640442 tensor([1, 9, 9, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 1, 9, 4, 3, 5, 8, 7, 6, 2], device='cuda:0')\n","205000 0.0 1.24647057056427 tensor([7, 8, 6, 7, 7, 6, 6, 8, 8, 8], device='cuda:0') tensor([0, 6, 7, 5, 1, 2, 4, 3, 9, 8], device='cuda:0')\n","210000 0.0 1.2024506330490112 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 8, 7, 3, 9, 1, 4, 2, 6], device='cuda:0')\n","215000 0.0 1.1216048002243042 tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 9, 3, 1, 4, 6, 2, 7, 5, 8], device='cuda:0')\n","220000 0.0 1.0853582620620728 tensor([3, 3, 7, 7, 7, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 2, 7, 9, 4, 5, 8, 1, 6, 3], device='cuda:0')\n","225000 0.0 1.0337178707122803 tensor([1, 2, 2, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 1, 2, 5, 8, 4, 6, 3, 7, 9], device='cuda:0')\n","230000 0.0 0.9653917551040649 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 8, 4, 7, 3, 1, 6, 5, 2, 9], device='cuda:0')\n","235000 0.0 0.9212439656257629 tensor([9, 9, 4, 4, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 1, 2, 7, 3, 9, 4, 6, 8, 5], device='cuda:0')\n","240000 0.0 0.9420247077941895 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 9, 4, 7, 5, 8, 3, 1, 2, 6], device='cuda:0')\n","245000 0.0 0.9107706546783447 tensor([4, 4, 4, 7, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 2, 9, 5, 8, 7, 4, 3, 1, 6], device='cuda:0')\n","250000 0.0 0.8397614359855652 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 7, 4, 8, 9, 2, 1, 3, 6], device='cuda:0')\n","255000 0.0 0.8131662607192993 tensor([9, 9, 9, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 1, 4, 8, 7, 2, 9, 3, 5, 6], device='cuda:0')\n","260000 0.0 0.7526090145111084 tensor([1, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 1, 3, 7, 9, 4, 5, 8, 6, 2], device='cuda:0')\n","265000 0.0 0.7802643179893494 tensor([2, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 3, 4, 2, 6, 8, 1, 7, 5, 9], device='cuda:0')\n","270000 0.0 0.6710699796676636 tensor([8, 7, 7, 7, 7, 8, 8, 8, 7, 7], device='cuda:0') tensor([0, 4, 6, 9, 3, 5, 2, 1, 8, 7], device='cuda:0')\n","275000 0.0 0.6983754634857178 tensor([3, 3, 3, 3, 3, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 2, 1, 4, 9, 5, 8, 6, 3, 7], device='cuda:0')\n","280000 0.0 0.6463814377784729 tensor([1, 8, 8, 8, 8, 8, 8, 8, 7, 7], device='cuda:0') tensor([0, 7, 1, 4, 9, 5, 2, 6, 3, 8], device='cuda:0')\n","285000 0.0 0.6287793517112732 tensor([2, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 7, 9, 1, 6, 3, 4, 5, 8, 2], device='cuda:0')\n","290000 0.0 0.6076962947845459 tensor([3, 3, 3, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 5, 1, 2, 8, 9, 3, 4, 7, 6], device='cuda:0')\n","295000 0.0 0.5966910719871521 tensor([6, 6, 6, 6, 6, 6, 7, 7, 7, 7], device='cuda:0') tensor([0, 2, 1, 7, 6, 3, 5, 9, 8, 4], device='cuda:0')\n","300000 0.0 0.5474984645843506 tensor([7, 6, 6, 6, 9, 9, 9, 6, 6, 6], device='cuda:0') tensor([0, 3, 2, 8, 4, 5, 1, 9, 7, 6], device='cuda:0')\n","305000 0.0 0.4864519536495209 tensor([1, 6, 5, 6, 5, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 1, 7, 3, 4, 2, 9, 5, 8, 6], device='cuda:0')\n","310000 0.0 0.45923078060150146 tensor([8, 8, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 3, 5, 8, 6, 4, 9, 2, 1, 7], device='cuda:0')\n","315000 0.0 0.44022059440612793 tensor([6, 7, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 7, 4, 8, 2, 9, 5, 1, 6, 3], device='cuda:0')\n","320000 0.0 0.4202153980731964 tensor([9, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 3, 1, 6, 7, 9, 8, 4, 2, 5], device='cuda:0')\n","325000 0.0 0.4971647262573242 tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 5, 1, 9, 2, 7, 6, 3, 8, 4], device='cuda:0')\n","330000 0.0 0.36162063479423523 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 6, 2, 7, 8, 3, 9, 5, 4, 1], device='cuda:0')\n","335000 0.0 0.3318961262702942 tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 3, 2, 6, 1, 7, 9, 5, 4, 8], device='cuda:0')\n","340000 0.0 0.31754013895988464 tensor([9, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 3, 7, 4, 8, 6, 2, 9, 1], device='cuda:0')\n","345000 0.0 0.2887006103992462 tensor([2, 7, 8, 7, 7, 7, 7, 7, 8, 8], device='cuda:0') tensor([0, 3, 5, 6, 1, 4, 8, 7, 2, 9], device='cuda:0')\n","350000 0.0 0.3294130265712738 tensor([2, 2, 3, 3, 4, 4, 9, 7, 9, 9], device='cuda:0') tensor([0, 1, 6, 3, 4, 2, 9, 7, 8, 5], device='cuda:0')\n","355000 0.0 0.2645365297794342 tensor([4, 4, 4, 4, 4, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 5, 4, 2, 1, 6, 7, 9, 3, 8], device='cuda:0')\n","360000 0.0 0.32284727692604065 tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 8], device='cuda:0') tensor([0, 5, 8, 4, 1, 3, 2, 6, 9, 7], device='cuda:0')\n","365000 0.0 0.22881601750850677 tensor([0, 6, 6, 6, 6, 6, 6, 6, 6, 4], device='cuda:0') tensor([0, 3, 6, 4, 9, 8, 2, 7, 5, 1], device='cuda:0')\n","370000 0.0 0.19852320849895477 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 8, 2, 9, 1, 4, 3, 6, 7, 5], device='cuda:0')\n","375000 0.0 0.18968452513217926 tensor([2, 7, 2, 2, 2, 2, 7, 7, 7, 7], device='cuda:0') tensor([0, 5, 3, 8, 4, 1, 2, 7, 9, 6], device='cuda:0')\n","380000 0.0 0.1834525316953659 tensor([8, 8, 8, 5, 5, 5, 5, 5, 6, 6], device='cuda:0') tensor([0, 2, 1, 3, 4, 5, 8, 6, 7, 9], device='cuda:0')\n","385000 0.0 0.16315943002700806 tensor([1, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 1, 7, 6, 9, 5, 2, 3, 4, 8], device='cuda:0')\n","390000 0.0 0.20400580763816833 tensor([8, 8, 8, 6, 6, 6, 9, 8, 2, 7], device='cuda:0') tensor([0, 2, 8, 6, 3, 4, 5, 9, 1, 7], device='cuda:0')\n","395000 0.0 0.19082055985927582 tensor([7, 5, 5, 7, 7, 7, 7, 7, 6, 6], device='cuda:0') tensor([0, 6, 4, 5, 1, 2, 9, 3, 8, 7], device='cuda:0')\n","400000 0.0 0.15907901525497437 tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 4, 9, 6, 5, 7, 3, 2, 1, 8], device='cuda:0')\n","405000 0.0 0.1244417130947113 tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 5, 1, 9, 2, 6, 4, 7, 3, 8], device='cuda:0')\n","410000 0.0 0.12342135608196259 tensor([8, 9, 3, 9, 3, 9, 9, 3, 3, 9], device='cuda:0') tensor([0, 4, 7, 6, 3, 2, 9, 8, 1, 5], device='cuda:0')\n","415000 0.0 0.11338993161916733 tensor([5, 5, 4, 4, 5, 4, 4, 5, 5, 5], device='cuda:0') tensor([0, 3, 1, 5, 2, 4, 8, 9, 7, 6], device='cuda:0')\n","420000 0.0 0.14706763625144958 tensor([7, 5, 6, 6, 4, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 9, 5, 7, 4, 1, 6, 8, 3, 2], device='cuda:0')\n","425000 0.0 0.11348289996385574 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 6, 4, 5, 7, 1, 3, 9, 2, 8], device='cuda:0')\n","430000 0.0 0.08826913684606552 tensor([1, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 1, 4, 6, 7, 2, 9, 8, 3, 5], device='cuda:0')\n","435000 0.0 0.08335178345441818 tensor([0, 2, 4, 4, 4, 4, 4, 4, 4, 2], device='cuda:0') tensor([0, 7, 9, 3, 8, 1, 6, 5, 4, 2], device='cuda:0')\n","440000 0.0 0.092147596180439 tensor([1, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 9, 1, 3, 4, 7, 6, 5, 8], device='cuda:0')\n","445000 0.0 0.09816879034042358 tensor([7, 4, 7, 4, 4, 4, 4, 7, 4, 4], device='cuda:0') tensor([0, 9, 3, 6, 1, 8, 4, 7, 5, 2], device='cuda:0')\n","450000 0.0 0.11150000244379044 tensor([3, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 5, 7, 3, 1, 4, 9, 8, 6], device='cuda:0')\n","455000 0.0 0.07834263890981674 tensor([1, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 1, 9, 3, 4, 7, 2, 8, 5, 6], device='cuda:0')\n","460000 0.0 0.06289873272180557 tensor([3, 6, 3, 3, 3, 3, 3, 3, 3, 4], device='cuda:0') tensor([0, 5, 1, 4, 6, 3, 2, 8, 7, 9], device='cuda:0')\n","465000 0.0 0.05774909630417824 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 4, 9, 1, 6, 7, 3, 5, 8, 2], device='cuda:0')\n","470000 0.0 0.05647699907422066 tensor([5, 3, 3, 3, 3, 8, 3, 3, 5, 5], device='cuda:0') tensor([0, 7, 9, 1, 3, 8, 5, 4, 6, 2], device='cuda:0')\n","475000 0.0 0.054360512644052505 tensor([2, 8, 8, 9, 9, 9, 8, 8, 8, 8], device='cuda:0') tensor([0, 1, 5, 6, 4, 7, 9, 8, 2, 3], device='cuda:0')\n","480000 0.0 0.09224975854158401 tensor([2, 8, 7, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 3, 4, 9, 6, 5, 8, 2, 1, 7], device='cuda:0')\n","485000 0.0 0.09147339314222336 tensor([5, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 3, 7, 6, 2, 9, 1, 5, 8, 4], device='cuda:0')\n","490000 0.0 0.10992752015590668 tensor([6, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 8, 4, 5, 6, 2, 7, 1, 3, 9], device='cuda:0')\n","495000 0.0 0.06864461302757263 tensor([8, 8, 8, 9, 8, 8, 8, 8, 4, 4], device='cuda:0') tensor([0, 4, 1, 8, 6, 9, 2, 7, 5, 3], device='cuda:0')\n","500000 0.0 0.05730339139699936 tensor([3, 3, 3, 3, 2, 2, 9, 9, 2, 2], device='cuda:0') tensor([0, 4, 9, 1, 2, 3, 8, 5, 6, 7], device='cuda:0')\n","505000 0.0 0.03997444733977318 tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 4, 8, 5, 9, 6, 1, 3, 2, 7], device='cuda:0')\n","510000 0.0 0.043805405497550964 tensor([4, 3, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 7, 1, 2, 5, 6, 4, 3, 9, 8], device='cuda:0')\n","515000 0.0 0.03653460741043091 tensor([8, 8, 5, 5, 5, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 2, 3, 6, 1, 9, 4, 7, 8, 5], device='cuda:0')\n","520000 0.0 0.03325710818171501 tensor([7, 5, 5, 5, 5, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 2, 5, 1, 7, 3, 9, 6, 4, 8], device='cuda:0')\n","525000 0.0 0.030088456347584724 tensor([3, 4, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 3, 9, 8, 1, 5, 7, 2, 4, 6], device='cuda:0')\n","530000 0.0 0.03188735619187355 tensor([6, 7, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 7, 4, 8, 2, 9, 5, 1, 6, 3], device='cuda:0')\n","535000 0.0 0.03056650422513485 tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 9, 5, 4, 3, 8, 6, 7, 2, 1], device='cuda:0')\n","540000 0.0 0.03543153405189514 tensor([7, 7, 7, 4, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 3, 7, 4, 9, 6, 2, 5, 8, 1], device='cuda:0')\n","545000 0.0 0.031682420521974564 tensor([5, 5, 5, 4, 4, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 6, 8, 5, 4, 2, 3, 1, 9, 7], device='cuda:0')\n","550000 0.0 0.025889134034514427 tensor([3, 7, 7, 7, 7, 7, 3, 4, 9, 9], device='cuda:0') tensor([0, 7, 6, 1, 5, 8, 3, 2, 4, 9], device='cuda:0')\n","555000 0.0 0.024852905422449112 tensor([6, 5, 7, 6, 6, 6, 6, 7, 7, 7], device='cuda:0') tensor([0, 4, 9, 3, 6, 1, 5, 7, 2, 8], device='cuda:0')\n","560000 0.0 0.024158280342817307 tensor([3, 5, 5, 5, 5, 2, 2, 6, 6, 6], device='cuda:0') tensor([0, 4, 6, 1, 5, 2, 9, 8, 3, 7], device='cuda:0')\n","565000 0.0 0.02133515477180481 tensor([3, 3, 3, 9, 7, 7, 9, 9, 2, 3], device='cuda:0') tensor([0, 2, 7, 1, 9, 3, 8, 4, 5, 6], device='cuda:0')\n","570000 0.0 0.020398952066898346 tensor([1, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0') tensor([0, 4, 1, 2, 6, 3, 9, 5, 8, 7], device='cuda:0')\n","575000 0.0 0.5188882946968079 tensor([1, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 6, 1, 7, 2, 5, 9, 8, 4, 3], device='cuda:0')\n","580000 0.0 0.14531756937503815 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 2, 3, 7, 6, 9, 8, 1, 4, 5], device='cuda:0')\n","585000 0.0 0.045719269663095474 tensor([2, 2, 4, 3, 5, 5, 4, 4, 5, 5], device='cuda:0') tensor([0, 2, 1, 3, 7, 9, 6, 8, 5, 4], device='cuda:0')\n","590000 0.0 0.02969774790108204 tensor([8, 8, 6, 8, 8, 5, 5, 6, 6, 6], device='cuda:0') tensor([0, 1, 9, 5, 8, 6, 7, 2, 3, 4], device='cuda:0')\n","595000 0.0 0.028644094243645668 tensor([1, 1, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 1, 7, 4, 5, 9, 8, 3, 2, 6], device='cuda:0')\n","600000 0.0 0.02351127378642559 tensor([8, 8, 8, 8, 8, 8, 8, 8, 2, 8], device='cuda:0') tensor([0, 2, 8, 5, 4, 6, 1, 9, 3, 7], device='cuda:0')\n","605000 0.0 0.05737202614545822 tensor([8, 6, 6, 4, 4, 4, 4, 6, 6, 6], device='cuda:0') tensor([0, 4, 6, 7, 5, 2, 3, 9, 1, 8], device='cuda:0')\n","610000 0.0 0.025143679231405258 tensor([8, 3, 3, 3, 3, 3, 3, 3, 8, 8], device='cuda:0') tensor([0, 2, 4, 6, 1, 5, 3, 7, 9, 8], device='cuda:0')\n","615000 0.0 0.019046703353524208 tensor([5, 5, 5, 4, 4, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 6, 8, 5, 4, 2, 3, 1, 9, 7], device='cuda:0')\n","620000 0.0 0.01773981563746929 tensor([8, 8, 9, 9, 9, 9, 8, 8, 8, 9], device='cuda:0') tensor([0, 3, 5, 9, 8, 7, 4, 1, 6, 2], device='cuda:0')\n","625000 0.0 0.017737004905939102 tensor([9, 4, 4, 4, 4, 9, 4, 4, 4, 3], device='cuda:0') tensor([0, 3, 4, 9, 8, 2, 1, 6, 7, 5], device='cuda:0')\n","630000 0.0 0.019420437514781952 tensor([8, 8, 8, 8, 8, 2, 2, 8, 8, 8], device='cuda:0') tensor([0, 3, 4, 9, 5, 7, 8, 1, 2, 6], device='cuda:0')\n","635000 0.0 0.015861650928854942 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 5, 7, 4, 8, 6, 3, 2, 1, 9], device='cuda:0')\n","640000 0.0 0.018386581912636757 tensor([5, 5, 5, 5, 5, 5, 8, 8, 8, 8], device='cuda:0') tensor([0, 4, 2, 9, 5, 1, 8, 3, 7, 6], device='cuda:0')\n","645000 0.0 0.016885509714484215 tensor([5, 5, 8, 8, 8, 8, 6, 6, 8, 8], device='cuda:0') tensor([0, 8, 4, 5, 2, 1, 6, 7, 3, 9], device='cuda:0')\n","650000 0.0 0.014345796778798103 tensor([2, 4, 4, 4, 4, 4, 4, 4, 8, 9], device='cuda:0') tensor([0, 1, 2, 4, 3, 7, 5, 6, 8, 9], device='cuda:0')\n","655000 0.0 0.013631594367325306 tensor([1, 8, 6, 6, 6, 8, 8, 8, 7, 6], device='cuda:0') tensor([0, 1, 3, 2, 5, 9, 4, 8, 6, 7], device='cuda:0')\n","660000 0.0 0.012873674742877483 tensor([1, 6, 6, 6, 6, 3, 6, 6, 6, 6], device='cuda:0') tensor([0, 5, 1, 6, 3, 8, 7, 9, 4, 2], device='cuda:0')\n","665000 0.0 0.013374373316764832 tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 7, 5, 2, 9, 4, 8, 6, 1, 3], device='cuda:0')\n","670000 0.0 0.01315551158040762 tensor([6, 6, 6, 6, 6, 6, 5, 5, 5, 5], device='cuda:0') tensor([0, 7, 2, 8, 4, 6, 5, 3, 1, 9], device='cuda:0')\n","675000 0.0 0.3291809856891632 tensor([5, 5, 3, 3, 3, 3, 3, 8, 8, 8], device='cuda:0') tensor([0, 6, 2, 4, 7, 1, 3, 5, 9, 8], device='cuda:0')\n","680000 0.0 0.09109095484018326 tensor([1, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 1, 7, 9, 3, 6, 8, 4, 5, 2], device='cuda:0')\n","685000 0.0 0.019111590459942818 tensor([1, 8, 8, 8, 8, 8, 8, 8, 4, 4], device='cuda:0') tensor([0, 4, 8, 1, 9, 5, 7, 3, 2, 6], device='cuda:0')\n","690000 0.0 0.023264024406671524 tensor([5, 5, 4, 4, 4, 4, 5, 5, 5, 5], device='cuda:0') tensor([0, 4, 8, 6, 9, 1, 2, 7, 3, 5], device='cuda:0')\n","695000 0.0 0.015409735962748528 tensor([4, 8, 4, 6, 6, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 3, 7, 6, 2, 8, 4, 9, 1, 5], device='cuda:0')\n","700000 0.0 0.014508223161101341 tensor([7, 7, 7, 7, 7, 7, 7, 7, 8, 8], device='cuda:0') tensor([0, 1, 9, 4, 8, 2, 7, 3, 5, 6], device='cuda:0')\n","705000 0.0 0.012712991796433926 tensor([8, 8, 9, 8, 6, 6, 9, 9, 9, 9], device='cuda:0') tensor([0, 6, 9, 2, 7, 4, 5, 3, 1, 8], device='cuda:0')\n","710000 0.0 0.013242761604487896 tensor([4, 5, 4, 4, 4, 4, 4, 4, 4, 5], device='cuda:0') tensor([0, 3, 7, 1, 4, 9, 5, 6, 2, 8], device='cuda:0')\n","715000 0.0 0.012946569360792637 tensor([5, 8, 3, 3, 3, 3, 8, 8, 3, 3], device='cuda:0') tensor([0, 3, 8, 5, 2, 9, 7, 1, 6, 4], device='cuda:0')\n","720000 0.0 0.011830941773951054 tensor([1, 9, 9, 9, 9, 8, 9, 7, 7, 7], device='cuda:0') tensor([0, 1, 9, 3, 6, 7, 4, 8, 5, 2], device='cuda:0')\n","725000 0.0 0.011477027088403702 tensor([1, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 1, 7, 4, 6, 9, 3, 2, 8, 5], device='cuda:0')\n","730000 0.0 0.010896582156419754 tensor([2, 3, 3, 3, 9, 9, 9, 9, 3, 3], device='cuda:0') tensor([0, 8, 1, 2, 9, 7, 3, 6, 4, 5], device='cuda:0')\n","735000 0.0 0.010185406543314457 tensor([2, 4, 4, 9, 4, 4, 4, 9, 9, 9], device='cuda:0') tensor([0, 4, 9, 2, 6, 1, 7, 3, 5, 8], device='cuda:0')\n","740000 0.0 0.008744487538933754 tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8], device='cuda:0') tensor([0, 8, 4, 5, 6, 9, 2, 3, 1, 7], device='cuda:0')\n","745000 0.0 0.007833868265151978 tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 2, 8, 9, 4, 5, 1, 7, 6, 3], device='cuda:0')\n","750000 0.0 0.0088497344404459 tensor([4, 7, 4, 4, 4, 3, 3, 3, 7, 7], device='cuda:0') tensor([0, 3, 7, 6, 2, 5, 9, 8, 1, 4], device='cuda:0')\n","755000 0.0 0.008864208124577999 tensor([2, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0') tensor([0, 6, 2, 1, 5, 7, 8, 3, 4, 9], device='cuda:0')\n","760000 0.0 0.008012416772544384 tensor([4, 5, 5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([0, 7, 6, 2, 9, 3, 8, 4, 5, 1], device='cuda:0')\n","765000 0.0 0.011510537005960941 tensor([1, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 1, 8, 2, 6, 4, 5, 7, 3, 9], device='cuda:0')\n","770000 0.0 0.23811200261116028 tensor([6, 6, 6, 6, 6, 9, 8, 8, 8, 8], device='cuda:0') tensor([0, 1, 7, 9, 3, 4, 2, 6, 8, 5], device='cuda:0')\n","775000 0.0 0.03067701682448387 tensor([7, 9, 9, 9, 7, 7, 7, 7, 7, 7], device='cuda:0') tensor([0, 1, 2, 3, 9, 7, 5, 8, 4, 6], device='cuda:0')\n","780000 0.0 0.016528958454728127 tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0') tensor([0, 8, 5, 4, 2, 3, 7, 1, 6, 9], device='cuda:0')\n","785000 0.0 0.01667749136686325 tensor([9, 8, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 3, 9, 8, 2, 5, 6, 7, 1, 4], device='cuda:0')\n","790000 0.0 0.011066209524869919 tensor([4, 9, 9, 9, 9, 9, 9, 9, 9, 9], device='cuda:0') tensor([0, 2, 5, 7, 3, 1, 4, 9, 8, 6], device='cuda:0')\n","795000 0.0 0.010123012587428093 tensor([3, 4, 4, 4, 4, 4, 8, 8, 8, 4], device='cuda:0') tensor([0, 5, 4, 8, 2, 3, 9, 1, 7, 6], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LdEboubnXXoj","colab_type":"text"},"source":["# LSTM Training"]},{"cell_type":"code","metadata":{"id":"vBs89uoNXPc_","colab_type":"code","outputId":"3b20303b-5803-4130-e402-49812afa8316","executionInfo":{"status":"ok","timestamp":1556836239079,"user_tz":240,"elapsed":1523780,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":459}},"source":["batch_size = 50\n","dataloader = DataLoader(tspdataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","hidden_size = 256\n","\n","pointer_net = Pointer_LSTM_Beam(hidden_size, batch_size)\n","\n","if torch.cuda.is_available():\n","    USE_CUDA = True\n","    print('Using GPU, %i devices.' % torch.cuda.device_count())\n","    pointer_net.cuda()\n","    net = torch.nn.DataParallel(pointer_net, device_ids=range(torch.cuda.device_count()))\n","    cudnn.benchmark = True\n","else:\n","    print(\"Not using GPU\")\n","    USE_CUDA = False\n","    \n","lr = 1e-3\n","    \n","CCE = torch.nn.CrossEntropyLoss()\n","pointer_optim = optim.RMSprop(filter(lambda p: p.requires_grad,\n","                            pointer_net.parameters()),\n","                     lr=lr)\n","    \n","def train(epochs=10, step=1000, name=\"model\"):\n","    losses = []\n","    batch_accs = []\n","    \n","    for epoch in range(epochs):\n","        for j, (input_tensor, target_tensor) in enumerate(dataloader):\n","           \n","            if USE_CUDA:\n","                input_tensor = input_tensor.cuda()\n","                target_tensor = target_tensor.cuda()\n","\n","            o, p = pointer_net(input_tensor)\n","            loss = CCE(o, target_tensor)\n","                        \n","            if ((len(dataloader))*(epoch) + (j+1)) % step == 0:\n","                \n","                _, p = pointer_net(input_tensor, True)\n","                if USE_CUDA:\n","                    p = p.cuda()\n","                \n","                compare = target_tensor == p\n","                acc_sum = compare.sum(dim=1)\n","                correct = 0.\n","                dist = compare[compare == 0].size()[0]\n","                for i in range(batch_size):\n","                    if acc_sum[i].item() == target_tensor.size(1):\n","                        correct += 1\n","                acc = correct / batch_size * 100\n","                batch_accs.append(acc)\n","                losses.append(loss.item())\n","                print(\"epoch {}: batch: {}/{} dist: {} loss: {}\".format(epoch+1,j+1,len(dataloader),dist,loss.item()), p[0], target_tensor[0])\n","                to_save = {\"pointer_net_state_dict\" : pointer_net.state_dict(), \"pointer_optim_state_dict\" : pointer_optim.state_dict(), \"epoch\" : epoch, \"j\" : j}\n","                torch.save(to_save, join(path, \"pointer_net_\"+name+\"_\"+str(epoch)+\".pt\"))\n","            \n","            pointer_optim.zero_grad()\n","            loss.backward()\n","#             torch.nn.utils.clip_grad_norm_(pointer_net.parameters(), 0.5)\n","            pointer_optim.step()\n","            \n","#     for l, a in zip(losses, batch_accs):\n","#         print(l,a)\n","\n","train(1000, step=20*40, name=\"masked_beam\")\n","            "],"execution_count":22,"outputs":[{"output_type":"stream","text":["Using GPU, 1 devices.\n","epoch 40: batch: 20/20 dist: 394 loss: 2.178508996963501 tensor([0, 3, 4, 1, 5, 2, 6, 8, 7, 9], device='cuda:0') tensor([0, 4, 8, 5, 7, 2, 3, 1, 6, 9], device='cuda:0')\n","epoch 80: batch: 20/20 dist: 370 loss: 2.1122777462005615 tensor([0, 1, 6, 3, 4, 8, 2, 9, 5, 7], device='cuda:0') tensor([0, 5, 1, 7, 8, 9, 2, 3, 4, 6], device='cuda:0')\n","epoch 120: batch: 20/20 dist: 379 loss: 2.0374844074249268 tensor([0, 2, 1, 5, 3, 4, 6, 8, 7, 9], device='cuda:0') tensor([0, 2, 1, 5, 8, 4, 9, 3, 6, 7], device='cuda:0')\n","epoch 160: batch: 20/20 dist: 377 loss: 2.0150139331817627 tensor([0, 7, 1, 5, 4, 9, 8, 2, 3, 6], device='cuda:0') tensor([0, 2, 7, 8, 9, 4, 3, 1, 6, 5], device='cuda:0')\n","epoch 200: batch: 20/20 dist: 367 loss: 1.963767170906067 tensor([0, 4, 1, 7, 9, 8, 2, 6, 3, 5], device='cuda:0') tensor([0, 2, 8, 5, 1, 9, 7, 3, 6, 4], device='cuda:0')\n","epoch 240: batch: 20/20 dist: 368 loss: 1.942868709564209 tensor([0, 1, 6, 4, 7, 3, 2, 9, 8, 5], device='cuda:0') tensor([0, 1, 5, 8, 3, 9, 2, 6, 4, 7], device='cuda:0')\n","epoch 280: batch: 20/20 dist: 380 loss: 1.9829071760177612 tensor([0, 8, 3, 4, 1, 5, 2, 9, 6, 7], device='cuda:0') tensor([0, 1, 4, 5, 9, 8, 3, 7, 2, 6], device='cuda:0')\n","epoch 320: batch: 20/20 dist: 362 loss: 1.9142353534698486 tensor([0, 6, 3, 1, 5, 7, 8, 4, 2, 9], device='cuda:0') tensor([0, 6, 1, 5, 7, 3, 4, 8, 2, 9], device='cuda:0')\n","epoch 360: batch: 20/20 dist: 374 loss: 1.8882086277008057 tensor([0, 6, 2, 4, 3, 9, 1, 8, 7, 5], device='cuda:0') tensor([0, 8, 6, 4, 3, 2, 9, 1, 7, 5], device='cuda:0')\n","epoch 400: batch: 20/20 dist: 362 loss: 1.8860129117965698 tensor([0, 2, 1, 7, 6, 4, 3, 8, 9, 5], device='cuda:0') tensor([0, 5, 2, 6, 1, 9, 4, 3, 7, 8], device='cuda:0')\n","epoch 440: batch: 20/20 dist: 371 loss: 1.8794472217559814 tensor([0, 4, 3, 1, 2, 9, 6, 5, 8, 7], device='cuda:0') tensor([0, 3, 4, 8, 1, 6, 7, 2, 9, 5], device='cuda:0')\n","epoch 480: batch: 20/20 dist: 375 loss: 1.8734612464904785 tensor([0, 7, 1, 3, 9, 2, 5, 8, 6, 4], device='cuda:0') tensor([0, 4, 9, 2, 3, 6, 8, 1, 7, 5], device='cuda:0')\n","epoch 520: batch: 20/20 dist: 368 loss: 1.8418394327163696 tensor([0, 1, 8, 2, 3, 7, 9, 4, 5, 6], device='cuda:0') tensor([0, 1, 9, 3, 8, 7, 5, 2, 4, 6], device='cuda:0')\n","epoch 560: batch: 20/20 dist: 363 loss: 1.8075056076049805 tensor([0, 1, 3, 5, 6, 2, 8, 7, 9, 4], device='cuda:0') tensor([0, 1, 9, 6, 5, 2, 4, 7, 8, 3], device='cuda:0')\n","epoch 600: batch: 20/20 dist: 373 loss: 1.8266631364822388 tensor([0, 1, 3, 4, 6, 2, 7, 5, 9, 8], device='cuda:0') tensor([0, 3, 9, 1, 5, 8, 4, 6, 2, 7], device='cuda:0')\n","epoch 640: batch: 20/20 dist: 373 loss: 1.8076831102371216 tensor([0, 3, 1, 5, 2, 7, 6, 8, 9, 4], device='cuda:0') tensor([0, 8, 4, 1, 9, 2, 3, 5, 7, 6], device='cuda:0')\n","epoch 680: batch: 20/20 dist: 379 loss: 1.7994813919067383 tensor([0, 9, 2, 3, 1, 6, 5, 4, 8, 7], device='cuda:0') tensor([0, 8, 2, 7, 3, 5, 1, 9, 6, 4], device='cuda:0')\n","epoch 720: batch: 20/20 dist: 381 loss: 1.7754749059677124 tensor([0, 3, 1, 5, 2, 6, 9, 4, 7, 8], device='cuda:0') tensor([0, 4, 5, 1, 2, 6, 9, 8, 3, 7], device='cuda:0')\n","epoch 760: batch: 20/20 dist: 371 loss: 1.7914435863494873 tensor([0, 4, 1, 8, 2, 5, 6, 9, 7, 3], device='cuda:0') tensor([0, 2, 5, 9, 1, 4, 6, 8, 3, 7], device='cuda:0')\n","epoch 800: batch: 20/20 dist: 363 loss: 1.7495747804641724 tensor([0, 6, 2, 8, 1, 4, 7, 9, 5, 3], device='cuda:0') tensor([0, 4, 1, 5, 7, 8, 9, 2, 3, 6], device='cuda:0')\n","epoch 840: batch: 20/20 dist: 375 loss: 1.7249466180801392 tensor([0, 8, 3, 1, 5, 6, 9, 7, 2, 4], device='cuda:0') tensor([0, 4, 6, 2, 9, 8, 3, 7, 1, 5], device='cuda:0')\n","epoch 880: batch: 20/20 dist: 356 loss: 1.7207109928131104 tensor([0, 9, 4, 2, 7, 5, 6, 8, 1, 3], device='cuda:0') tensor([0, 8, 3, 9, 2, 5, 6, 4, 7, 1], device='cuda:0')\n","epoch 920: batch: 20/20 dist: 378 loss: 1.7505592107772827 tensor([0, 1, 4, 3, 5, 6, 8, 2, 7, 9], device='cuda:0') tensor([0, 7, 1, 3, 2, 4, 5, 8, 6, 9], device='cuda:0')\n","epoch 960: batch: 20/20 dist: 374 loss: 1.6873786449432373 tensor([0, 2, 5, 7, 4, 8, 3, 9, 6, 1], device='cuda:0') tensor([0, 5, 9, 7, 3, 4, 8, 2, 1, 6], device='cuda:0')\n","epoch 1000: batch: 20/20 dist: 362 loss: 1.6833927631378174 tensor([0, 2, 4, 3, 5, 9, 1, 6, 8, 7], device='cuda:0') tensor([0, 4, 1, 6, 5, 9, 7, 8, 2, 3], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UFLmpGmGaj2z","colab_type":"code","colab":{}},"source":["torch.save(pointer_net.state_dict(), join(path,\"masked_pointernet_final.pt\"))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFRw6CZAaDu2","colab_type":"code","outputId":"4b4e3eba-5c8f-405b-e758-b3a00a3b1b11","executionInfo":{"status":"ok","timestamp":1556839561399,"user_tz":240,"elapsed":7357,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["total_missed = 0.\n","total_dist = 0.\n","\n","opt_total = 0.\n","\n","dataloader = DataLoader(tspdataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","for j, (input_tensor, target_tensor) in enumerate(dataloader):\n","           \n","    if USE_CUDA:\n","        input_tensor = input_tensor.cuda()\n","        target_tensor = target_tensor.cuda()\n","\n","    _, p = pointer_net(input_tensor, True)\n","    if USE_CUDA:\n","        p = p.cuda()\n","    \n","#     print(input_tensor.size(), target_tensor.size())\n","\n","    compare = target_tensor == p\n","    acc_sum = compare.sum(dim=1)\n","    dist = compare[compare == 0].size()[0]\n","    total_missed += dist\n","    \n","    tour_dist = 0.\n","    opt_dist = 0.\n","    \n","    for k in range(50):\n","#         print(input_tensor[k,])\n","        for i in range(9):\n","\n","#             print(input_tensor[k,p[k,i],:], input_tensor[k,p[k,i+1],:])\n","#             print(p[k,i])\n","            x = torch.pow(input_tensor[k,p[k,i],0] - input_tensor[k,p[k,i+1],0],2)\n","            y = torch.pow(input_tensor[k,p[k,i],1] - input_tensor[k,p[k,i+1],1],2)\n","            td = torch.sqrt(x + y)\n","            \n","            x_opt = torch.pow(input_tensor[k,target_tensor[k,i],0] - input_tensor[k,target_tensor[k,i+1],0],2)\n","            y_opt = torch.pow(input_tensor[k,target_tensor[k,i],1] - input_tensor[k,target_tensor[k,i+1],1],2)\n","            td_opt = torch.sqrt(x_opt + y_opt)\n","#             print(td)\n","\n","            tour_dist += td\n","            opt_dist += td_opt\n","        \n","    total_dist += tour_dist\n","    opt_total += opt_dist\n","    \n","print(total_dist / (50. * len(dataloader)))\n","print(opt_total / (50. * len(dataloader)))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","torch.Size([50, 10, 2]) torch.Size([50, 10])\n","tensor(47.8736, device='cuda:0')\n","tensor(27.3511, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EulSVbG-Ftph","colab_type":"code","outputId":"6714a331-0991-40e8-ac1a-9ddfeb7983b8","executionInfo":{"status":"ok","timestamp":1556850792731,"user_tz":240,"elapsed":483,"user":{"displayName":"Rahul Kharse","photoUrl":"https://lh3.googleusercontent.com/-kAvRFISf0mk/AAAAAAAAAAI/AAAAAAAAAOE/o9zCtsUPs28/s64/photo.jpg","userId":"01741049863904883701"}},"colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["X = [40, 80, 120, 160, 200, 240, 280, 320, 360, 400, 440, 480, 520, 560, 600, 640, 680, 720, 760, 800, 840, 880, 920, 960, 1000]\n","Y = [2.178508996963501, 2.1122777462005615, 2.0374844074249268, 2.0150139331817627, 1.963767170906067, 1.942868709564209, 1.9829071760177612, 1.9142353534698486, 1.8882086277008057, 1.8860129117965698, 1.8794472217559814, 1.8734612464904785, 1.8418394327163696, 1.8075056076049805, 1.8266631364822388, 1.8076831102371216, 1.7994813919067383, 1.7754749059677124, 1.7914435863494873, 1.7495747804641724, 1.7249466180801392, 1.7207109928131104, 1.7505592107772827, 1.6873786449432373, 1.6833927631378174]\n","\n","import matplotlib.pyplot as plt\n","\n","plt.title(\"Modified Beam Search Loss\")\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.plot(X,Y)\n"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f7b9fec3eb8>]"]},"metadata":{"tags":[]},"execution_count":1},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}